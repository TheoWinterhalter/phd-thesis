% \setchapterpreamble[u]{\margintoc}
\chapter{Reduction}
\labch{coq-reduction}

A key element to defining a type checker is to have a conversion checker.
Conversion in turn relies on reduction. The reduction I describe in
\nrefch{coq-spec} is just a relation and is non-deterministic. In this chapter
I will present an algorithmic implementation of reduction.
This implementation relies on the axiom of strong normalisation, but even then
it requires some more work to show the process is terminating.

\section{Weak head normalisation}

Since reduction is non deterministic, we have to pick a strategy to implement.
When checking terms for conversion, one thing we want to do is verify that they
have the same head constructor: if two terms are \(\lambda\)-abstractions
\(\lambda (x:A).t\) and \(\lambda (x:A').t'\) we need only compare \(A\) and
\(A'\), as well as \(t\) and \(t'\) for conversion.
Weak head reduction is a strategy that allows us to access the head of a term
rather efficiently.

\subsection{Characterisation}

The idea is to only deal with redexes (\ie left-hand side of reduction rules)
that appear at the top-level and might be hiding the head constructor. As such
when considering \(\lambda (x:A).t\), neither \(A\) nor \(t\) will be reduced
because we already know the head, or the shape, of the term: it is a
\(\lambda\). However, if we have an application \(t\ u\), we will first weak
head reduce \(t\) to see if it reaches a \(\lambda\): if it does we substitute
\(u\) and repeat the process, if it does not, then we have reached a weak head
normal form.

Weak head normal forms are by definition the terms that cannot be reduced
further by weak head reduction.

\begin{definition}[Normal form]
  A term \(t\) is a normal form for a reduction \(\red\) if it doesn't reduce
  for \(\red\).
  \[
    t \not \red
  \]
\end{definition}

It is however possible to characterise weak
head normal forms syntactically. For this we need to talk about \emph{neutral}
forms.
The intuition is that a neutral term is a \emph{stuck} term.

\begin{definition}[Neutral forms]
  A term \(t\) is neutral for a reduction \(\red\) if substituting it inside
  another term doesn't introduce redexes. Equivalently substituting a neutral
  term \(t\) in a normal form \(u\) will yield another normal form.
  \[
    u[x \sto t] \not \red
  \]
\end{definition}

A neutral form is in particular a normal form.

Focusing on a small fragment of \acrshort{PCUIC} without inductive types and so
on, we can characterise weak head neutral (\(\whne\)) and normal forms \(\whnf\)
with the following mutual judgements:
\marginnote[1cm]{
  The first rule is there to make sure that the variable doesn't correspond to
  local definition because those reduce.
}
\begin{mathpar}
  \infer
    {(x : A) \in \Ga}
    {\Ga \vdash x\ \whne}
  %

  \infer
    {\Ga \vdash u\ \whne}
    {\Ga \vdash u\ v\ \whne}
  %

  \infer
    {\Ga \vdash u\ \whne}
    {\Ga \vdash u\ \whnf}
  %

  \infer
    { }
    {\Ga \vdash \Type\ \whnf}
  %

  \infer
    { }
    {\Ga \vdash \Pi (x.A).B\ \whnf}
  %

  \infer
    { }
    {\Ga \vdash \lambda (x.A).t\ \whnf}
  %
\end{mathpar}
As you can see, terms like \((\lambda (x.A).t)\ u\) or
\(\tlet\ x := u\ \tin\ t\) are not weak head normal forms, and a fortiori not
weak head neutral forms.
Indeed they both reduce to \(t[x \sto u]\) for weak head reduction.

Before I show how we deal with rest of the constructions in the formalisation
I need to talk about how we parameterise the weak head reduction.

\subsection{Reduction flags}

In order to be able to fine-tune our reduction strategy we introduce the notion
of reduction flags. Those mimic the flags that are used by \Coq internally.

\begin{minted}{coq}
Module RedFlags.

  Record t := mk {
    beta   : bool ;
    iota   : bool ;
    zeta   : bool ;
    delta  : bool ;
    fix_   : bool ;
    cofix_ : bool
  }.

  Definition default := mk true true true true true true.

  Definition nodelta := mk true true true false true true.

End RedFlags.
\end{minted}

The names refer to the reduction rules that they activate or not.
Setting \mintinline{coq}{beta} to \mintinline{coq}{true} means that we will
reduce \(\beta\)-redexes.
\mintinline{coq}{iota} is for the reduction of pattern-matching,
\mintinline{coq}{zeta} for the reduction of let-bindings and unfolding of local
definitions, \mintinline{coq}{delta} is for the unfolding of global definitions,
\mintinline{coq}{fix_} is for the unfolding of fixed-points and
\mintinline{coq}{cofix_} for that of cofixed-points.

The default is to reduce all those, but later we will also use a version which
doesn't do \(\delta\)-reductions to speed up conversion.

\subsection{\Coq specification}

The reduction flags parameterise the weak head reduction and as such they
affect the definition of weak head normal and neutral forms.

\marginnote[1cm]{
  The global context and reduction flags are assumed globally in the definition
  using the commands \mintinline{coq}{Context (flags : RedFlags.t).}
  and \mintinline{coq}{Context (Σ : global_env).}
}
\begin{minted}{coq}
Inductive whnf Γ : term -> Prop :=
| whnf_ne t            : whne Γ t -> whnf Γ t
| whnf_sort s          : whnf Γ (tSort s)
| whnf_prod na A B     : whnf Γ (tProd na A B)
| whnf_lam na A B      : whnf Γ (tLambda na A B)
| whnf_cstrapp i n u v : whnf Γ (mkApps (tConstruct i n u) v)
| whnf_indapp i u v    : whnf Γ (mkApps (tInd i u) v)
| whnf_cofix mfix idx  : whnf Γ (tCoFix mfix idx)
| whnf_fix_short mfix idx narg body args :
    unfold_fix mfix idx = Some (narg, body) ->
    nth_error args narg = None ->
    whnf Γ (mkApps (tFix mfix idx) args)

with whne Γ : term -> Prop :=
| whne_rel i :
    option_map decl_body (nth_error Γ i) = Some None ->
    whne Γ (tRel i)

| whne_rel_nozeta i :
    RedFlags.zeta flags = false ->
    whne Γ (tRel i)

| whne_letin_nozeta na B b t :
    RedFlags.zeta flags = false ->
    whne Γ (tLetIn na B b t)

| whne_const c u decl :
    lookup_env Σ c = Some (ConstantDecl decl) ->
    decl.(cst_body) = None ->
    whne Γ (tConst c u)

| whne_const_nodelta c u :
    RedFlags.delta flags = false ->
    whne Γ (tConst c u)

| whne_fix mfix idx narg body args a :
    unfold_fix mfix idx = Some (narg, body) ->
    nth_error args narg = Some a ->
    whne Γ a ->
    whne Γ (mkApps (tFix mfix idx) args)

| whne_fix_nofix_ mfix idx args :
    RedFlags.fix_ flags = false ->
    whne Γ (mkApps (tFix mfix idx) args)

| whne_cofix_nocofix_ mfix idx args :
    RedFlags.cofix_ flags = false ->
    whne Γ (mkApps (tCoFix mfix idx) args)

| whne_app f v :
    whne Γ f ->
    whne Γ (tApp f v)

| whne_case i p c brs :
    whne Γ c ->
    whne Γ (tCase i p c brs)

| whne_case_noiota i p c brs :
    RedFlags.iota flags = false ->
    whne Γ (tCase i p c brs)

| whne_proj p c :
    whne Γ c ->
    whne Γ (tProj p c)

| whne_proj_noiota p c :
    RedFlags.iota flags = false ->
    whne Γ (tProj p c).
\end{minted}

Applied constructors and inductive types are normal, same as cofixed-points.
A fixed-point can be normal even if applied to arguments, only the recursive
argument must not be provided (because when the recursive argument is a
constructor, the fixed-point can be unfolded).
If the recursive argument is provided and is neutral then the fixed-point is
neutral.
It is also considered neutral when the corresponding flag \mintinline{coq}{fix_}
is set to \mintinline{coq}{false}.

Variables are neutral if they refer to local definitions when \(\zeta\) is
deactivated, or if they refer to assumptions.
Similarly, constants are neutral is they refer to global definitions when
\(\delta\) is off, or if they refer to an axiom.
let-bindings are only neutral when \(\zeta\) is off.
A pattern-matching expression is neutral when the scrutinee is neutral or if the
\(\iota\) flag is off. The same goes for projections.

This definition can serve as a specification for the weak head reduction
procedure we implement.

\section{The reduction machine}

We now have a clearer idea of what we wish to implement. As I already said, this
is still not as straightforward as one would hope.
As it is a bit more complex than an evaluation function taking a term and
returning its weak head normal form, we talk about an \emph{abstract machine}.

\subsection{Idea behind the machine}

The idea behind the machine is to \emph{focus and reduce}: whenever there is
a redex at the top-level, we reduce it, and when we reach a term potentially
stuck (say an application), we focus on a subterm and reduce it to see if we
reach another redex.

This is often achived by taking a list of arguments, \ie a \emph{stack}, besides
the term. When considering an application, the argument is \emph{pushed} on top
of the stack and we \emph{focus} on the left-hand term.
I will write \(\dpair{t \mid \pi}\) for term \(t\) \emph{against} stack \(\pi\).
We want to define weak head reduction \(\red_\WM\) as something like this:
\marginnote[1cm]{
  Here I confuse \(\dpair{t \mid \pi}\) as the data of a pair and as the term
  corresponding to \(t\) applied to the arguments of \(\pi\).
}
\begin{mathpar}
  \infer
    {\Ga \vdash \dpair{t \mid u :: \pi} \red_\WM a}
    {\Ga \vdash \dpair{t\ u \mid \pi} \red_\WM a}
  %

  \infer
    {\Ga \vdash \dpair{t[x \sto u] \mid \pi} \red_\WM a}
    {\Ga \vdash \dpair{\lambda (x:A).t \mid u :: \pi} \red_\WM a}
  %

  \infer
    {\Ga \vdash \dpair{t \mid \pi}\ \whnf}
    {\Ga \vdash \dpair{t \mid \pi} \red_\WM \dpair{t \mid \pi}}
  %
\end{mathpar}
The things above the line can be seen as recursive calls of the machine.
This is rather informal, but it doesn't correspond to the final definition
anyway.

Regarding termination, we see two cases: in the first we focusb on a subterm,
whereas in the second we do a recursive call on a \emph{reduct}.
In other words we need a well-founded order that satisfies
\[
  \begin{array}{rcl}
    \dpair{t \mid u :: \pi} &\prec& \dpair{t\ u \mid \pi} \\
    \dpair{t[x \sto u] \mid \pi} &\prec& \dpair{\lambda (x:A).t \mid u :: \pi}
  \end{array}
\]

As we have seen, assuming strong normalisation is tantamount to having a
well-founded order on coreduction, and it shouldn't be surprising that the
subterm relation is also well-founded. Taking a lexicographical order on both
these orders seems like a good option.

Remember that lexicographical orders operate on pairs and that the first
components mus be \emph{equal} when using the order on the second components.
Thus we must produce a pair from the data \(\dpair{t \mid \pi}\), coreduction is
to be taken on the whole term (\(t\) applied to \(\pi\)), while the subterm
relation only looks at \(t\).
This forces us to consider coreduction before the subterm relation, indeed if we
were to consider the pair \((t, \dpair{t \mid \pi})\) we would indeed satisfy
the first inequality as \(t\) is a subterm of \(t\ u\), but in the second
inequality the first components wouldn't be equal.
We will thus consider the following pair:
\[
  (\dpair{t \mid \pi},\ t)
\]
with the corresponding lexicographical order of coreduction and subterm
relation. This one works because \(\dpair{t \mid u :: \pi}\) and
\(\dpair{t\ u \mid \pi}\) represent the same term so that the first components
are indeed equal when consdering the subterm order.
\marginnote[1cm]{
  I write \(x \sqsubset y\) to mean that \(x\) is a subterm of \(y\).
}
\[
  \begin{array}{rcl}
    (\dpair{t \mid u :: \pi},\ t) &\prec& (\dpair{t\ u \mid \pi},\ t\ u) \\
    \text{since} &
    \multicolumn{2}{l}{\dpair{t \mid u :: \pi} = \dpair{t\ u \mid \pi}} \\
    \text{and} &
    \multicolumn{2}{l}{t \sqsubset t\ u} \\
    \\
    (\dpair{t[x \sto u] \mid \pi},\ t[x \sto u]) &\prec&
    (\dpair{\lambda (x:A).t \mid u :: \pi},\ \lambda (x:A).t) \\
    \text{since} &
    \multicolumn{2}{l}{
      \dpair{\lambda (x:A).t \mid u :: \pi} \red \dpair{t[x \sto u] \mid \pi}
    }
  \end{array}
\]

Problems arise however when considering other syntactic constructions of
\acrshort{PCUIC}, particularly pattern-matching and fixed-points.
I will identify two such problems before finally moving on to the next section
where I introduce the order we will actually use.

\paragraph{Problem 1:}
List of arguments as stacks will prove to be a problem when focusing on
something else than a function. This is the case when reducing pattern-matching
and projections. I will use projections on pairs as an example.
\[
  \infer
    {
      \Ga \vdash \dpair{p \mid \ctxempty} \red_\WM \dpair{(u,v) \mid \ctxempty} \\
      \Ga \vdash \dpair{u \mid \pi} \red_\WM a
    }
    {\Ga \vdash \dpair{p.1 \mid \pi} \red_\WM a}
\]
The idea is that to reduce \(p.1\) (potentially applied to arguments) we want to
reduce \(p\) (this time it is not applied) to see if it is a constructor of the
record, \ie a pair.
\marginnote[0.2cm]{
  We also need
  \[
    ((u,v),\ (u,v)) \prec (\dpair{p.1 \mid \pi},\ p.1)
  \]
  It holds because \(u\) is a reduct of \(p.1\). To know this we need to know
  that the reduction machine is correct while proving it terminates. I will say
  more about this later.
}
We now need
\[
  (\dpair{p \mid \ctxempty},\ p) \prec (\dpair{p.1 \mid \pi},\ p.1)
\]
However \(p\) is not a reduct of \(\dpair{p.1 \mid \pi}\), and is not equal to
it so we cannot use the fact that \(p\) is a subterm of \(p.1\).
The solution I propose to this is to actually remember the full term we were
considering before focusing. This is what lists of arguments were achieving
when considering only \(\lambda\)-abstraction and application.
For this we can use the stacks I introduced in \nrefch{coq-positions}.
\begin{mathpar}
  \infer
    {\Ga \vdash \dpair{t \mid \stack{\shole\ u} :: \pi} \red_\WM a}
    {\Ga \vdash \dpair{t\ u \mid \pi} \red_\WM a}
  %

  \infer
    {\Ga \vdash \dpair{t[x \sto u] \mid \pi} \red_\WM a}
    {
      \Ga \vdash
      \dpair{\lambda (x:A).t \mid \stack{\shole\ u} :: \pi}
      \red_\WM a
    }
  %

  \infer
    {\Ga \vdash \dpair{t \mid \pi}\ \whnf}
    {\Ga \vdash \dpair{t \mid \pi} \red_\WM \dpair{t \mid \pi}}
  %

  \infer
    {
      \Ga \vdash
      \dpair{p \mid \stack{\shole.1} :: \pi} \red_\WM
      \dpair{(u,v) \mid \stack{\shole.1} :: \pi} \\
      \Ga \vdash \dpair{u \mid \pi} \red_\WM a
    }
    {\Ga \vdash \dpair{p.1 \mid \pi} \red_\WM a}
  %
\end{mathpar}
This time, as the stack is preserved, the first components are indeed the same
and we have the inequality.
\[
  (\dpair{p \mid \stack{\shole.1} :: \pi},\ p)
  \prec
  (\dpair{p.1 \mid \pi},\ p.1)
\]

\paragraph{Problem 2:}
Using stacks solves our first problem but there remains another when considering
fixed-points this time. Indeed, fixed-points can only be unfolded when their
recusrive argument is a constructor. This means that when reducing a fixed-point
we have to focus on the recursive argument which is \emph{on} the stack:
\marginnote[0.5cm]{
  I here use non-mutual fixed-points to make things simpler.
  Remember here that \(n\) is the index of the recursive argument of the
  fixed-point.
}
\[
  \infer
    {
      \begin{array}{ll}
        \Ga \vdash &
        \vscmd
          {u_n}
          {
            \stack{(\fixp_n f.\ t)\ u_1\ \dots\ u_{n-1}\ \shole}
            :: \pi
          }
        \arcr
        \red_\WM &
        \vscmd
          {\mathsf{C}}
          {
            \stack{(\fixp_n f.\ t)\ u_1\ \dots\ u_{n-1}\ \shole}
            :: \pi
          }
      \end{array}
      \\
      \Ga \vdash
      \vscmd
        {t[f \sto \fixp_n f.\ t]}
        {
          \coapp{u_1} :: \dots :: \coapp{u_{n-1}} :: \pi
        }
      \red_\WM
      a
    }
    {
      \Ga \vdash
      \vscmd
        {\fixp_n f.\ t}
        {\coapp{u_1} :: \dots :: \coapp{u_{n-1}} :: \coapp{u_n} :: \pi}
      \red_\WM
      a
    }
  %
\]

\subsection{The order}

\subsection{The \Coq formalisation}