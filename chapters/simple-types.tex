% \setchapterpreamble[u]{\margintoc}
\chapter{Simple type theory}
\labch{simple-types}

When studying programming languages theoretically, \(\lambda\)-calculus imposes
itself as the prototypical example. A model of computation much simpler than
Turing machines, it becomes extremely useful in combination with a so-called
type system. This combination culminates into the Curry-Howard isomorphism that
relates programming and logic in profound ways, serving as a foundation for
type theory and modern logic.

I will not do a thorough analysis and history of the subject but I will give an
account of my understanding of it, limiting myself to points that I find
relevant to my thesis. The interested reader can refer
to~\sidecite{cardone2006history}.

\section{The \(\lambda\)-calculus}

\(\lambda\)-calculus can be reasonably called the simplest programming language.
It consists basically of functions, variables and applications.
If you are familiar with \ocaml these constructs are summarised in the example
below.
\begin{minted}{ocaml}
(fun x -> x) u
\end{minted}
Here, we have the identity function \mintinline{ocaml}{fun x -> x}---\ie the
function which maps \mintinline{ocaml}{x} to \mintinline{ocaml}{x}---applied to
some expression \mintinline{ocaml}{u}.
In mathematical textbooks, we would define the identity function (for natural
numbers) as follows.
\[
  \mathit{id} :
  \left(
  \begin{array}{lcl}
    \mathbb{N} &\to& \mathbb{N} \\
    x &\mapsto& x
  \end{array}
  \right)
\]
The whole expression would be written \(\mathit{id}(u)\).

The \(\lambda\)-calculus provides a third way of writing the same thing.
\[
  (\lambda x.\ x)\ u
\]
The little \(\lambda\) corresponds to the declaration of a function, in this
case \emph{binding} the \(x\) before the dot, in the expression after it.
Application of a function is marked with a space, such as \(u\ v\) meaning
\(u\) applied to argument \(v\).
Formally, the grammar of \(\lambda\)-calculus is:
\marginnote[0.55cm]{
  \(x\) is a placeholder for any variable name, typicaly in the range of \(x\),
  \(y\) and \(z\).
}
\[
  t, u, v \coloneqq x \mid \lambda x.\ t \mid t\ u
\]
and that is it.

In a term (the expressions of \(\lambda\)-calculus), the variable names are
considered irrelevant, for instance, the \(\lambda\)-terms \(\lambda x.\ x\)
and \(\lambda y.\ y\) are deemed equivalent, because they both define the
identity function.
% Of course, it only works, if all occurrences of \(x\) are replaced consistently
% with \(y\).
The operation replacing \(x\) for \(y\) in the term is called
\(\alpha\)-renaming; we thus talk about \(\alpha\)-equality:
\[
  \lambda x.\ x =_\alpha \lambda y.\ y
\]

Of course, that alone is not sufficient to describe a programming language,
it is missing a key component: evaluation of programs.
Indeed, without it, variables are meaningless.

We want to say that the application of a function should always yield some
result. For instance, the identity function \(\lambda x.\ x\) when applied to
\(u\) should naturally reduce to \(u\) itself.

The purpose of a variable is to be \emph{instantiated}.
Before we talk about this, we need to talk about \emph{bound} and \emph{free}
variables.
If you take the expression \((\lambda x.\ x)\ y\) you can see two variables
\(x\) and \(y\); the two do not behave the same: \(x\) is below a
\(\lambda\)-abstraction that \emph{introduces} (or \emph{binds}) it whereas
\(y\) is \emph{free}, no \(\lambda\)-abstraction constrains it.
Of course, this status changes if the term is put inside another:
\[
  \lambda y.\ (\lambda x.\ x)\ y
\]
This times both \(y\) and \(x\) are \emph{bound}.

A term with free variables is called \emph{open}, while a term with only bound
variables is called \emph{closed}.

\marginnote[0.2cm]{
  Sometimes this fact is summarised in the term \emph{capture-avoiding} meaning
  that bound variables are not touched. I will refrain from using a qualifier
  that basically means that it is not a nonsensical definition.
}
The only variables that can be instantiated are the \emph{free} ones.
For example, it would not make sense to replace the \(x\) in the identity
\(\lambda x.\ x\) function, it would somehow break it. \emph{Substitution} is
the operation that replaces free variables with other expressions.
We will write
\[
  t[x \sto u]
\]
to mean the term \(t\) were all \emph{free} occurrences of the variable \(x\)
are replaced by the term \(u\).

For example,
\[
  ((\lambda x.\ x)\ y)[y \sto u] = (\lambda x.\ x)\ u
\]

Now that we have substitution, we are armed to deal with reduction.
Reduction is defined from the so-called \(\beta\)-reduction
\[
  (\lambda x.\ t)\ u \red_\beta t[x \sto u]
\]
It is essentially saying that when applied to an argument, a function
reduces to its body were the argument replaces the variable it was binding.
Reduction \(\red\) is then obtained by taking the contextual closure of
\(\red_\beta\), \ie allowing reduction in each subterm:
\begin{mathpar}
  \infer
    {u \red u'}
    {u\ v \red u'\ v}
  %

  \infer
    {v \red v'}
    {u\ v \red u\ v'}
  %

  \infer
    {t \red t'}
    {\lambda x.\ t \red \lambda x.\ t'}
  %

  \infer
    { }
    {(\lambda x.\ t)\ u \red t[x \sto u]}
  %
\end{mathpar}

There is a lot more to say on the \emph{untyped} \(\lambda\)-calculus, but this
out of scope of this document.

\section{Types for programs}

Not all programs make sense. Consider the term
\[
  \delta \coloneqq \lambda x.\ x\ x
\]
giving \(x\) as argument to \(x\) should already feel wrong somehow, but if you
give \(\delta\) as argument to itself you get
\[
  \Omega \coloneqq \delta\ \delta
\]
Now let us have a look at its computational behaviour:
\[
  \begin{array}{lcl}
    \Omega &\coloneqq& \delta\ \delta \\
    &=& (\lambda x.\ x\ x)\ \delta \\
    &\red_\beta& \delta\ \delta \\
    &=& \Omega
  \end{array}
\]
The problem should become apparent: \(\Omega\) reduces to itself!
This means that the evaluation of \(\Omega\) will not terminate.
Generally, that is not something you want to have.
In case you have more structure in your language, such as in \ocaml where you
have integers, you also want to restrict some operations like addition to
expressions that are of the right \emph{kind}---in our case, integers.
In \ocaml, expressions like \mintinline{ocaml}{true + 3} or
\mintinline{ocaml}{1 + fun x -> 2} should be---and are!---rejected.

The way we prevent such problematic terms---\ie those that do not have
semantics---is by using \emph{types}. The \(\lambda\)-calculus is pretty simple,
the only things we can do is define functions and apply them, as such we
introduce a type of functions \(A \to B\).
The expression \(A \to B\) denotes the type of functions that take an argument
of type \(A\) and produce a value of type \(B\).
If we go back to \ocaml for a bit, a function like
\mintinline{ocaml}{fun x -> x + x} will have type \mintinline{ocaml}{int -> int}
while another making use of different data structures like
\begin{minted}{ocaml}
fun b -> if b then "Yes" else "No"
\end{minted}
will be of type
\mintinline{ocaml}{bool -> string}.
Going back to \(\lambda\)-calculus, the important rule is that when you have
a function \(f\) of type \(A \to B\)---which we will write \(f : A \to B\)---and
an argument \(u : A\), the result of the application will have type \(B\):
\[
  \infer
    {
      f : A \to B \\
      u : A
    }
    {f\ u : B}
  %
\]

The usefulness of types is best summarised in the famous quote:
\begin{quote}
  ``Well-typed programs cannot go wrong.''

  \hspace*{\fill} --- Robin Milner~\sidecite{milner1978theory}
\end{quote}
A program can `go wrong' if it needs to evaluate a nonsensical expression like
\(1\) applied to \(0\) which might have unexpected behaviours on a computer.

As I mentioned earlier, \(\lambda\)-terms can sometimes be open terms---\ie
contain free variables---so in order to type them, we need to know the type of
those variables. This information is called an environment or \emph{context}.
We can thus give the syntax of types and contexts.
\marginnote[0.8cm]{
  \(o\) stands for base types. A context is a list of type assignments for the
  variables, implicitly the variables are distinct.
}
\[
  \begin{array}{rcl}
    A, B &\bnf& o \bnfor A \to B \\
    \Ga, \D &\bnf& \ctxempty \bnfor \Ga, x : A
  \end{array}
\]

The typing rules of the \acrfull{STL} are the following.
\begin{mathpar}
  \infer
    {\Ga, x:A \vdash t : B}
    {\Ga \vdash \lambda x.\ t : A \to B}
  %

  \infer
    {
      \Ga \vdash u : A \to B \\
      \Ga \vdash v : A
    }
    {\Ga \vdash u\ v : B}
  %

  \infer
    {(x : A) \in \Ga}
    {\Ga \vdash x : A}
  %
\end{mathpar}

\marginnote[0.2cm]{
  See \nrefch{desirable-props} for more on this.
}
There are many interesting properties that \acrshort{STL} enjoys, one of the
most important is that it is terminating, \ie reduction cannot go indefinitely.
This excludes \emph{bad} terms like \(\Omega\).

\section{The Curry-Howard isomorphism}

Now, if we look back on the rules I gave for implication in
\nrefch{proof-theory}, there is a parallel with the typing rules of
\acrshort{STL}.
\begin{mathpar}
  \infer
    {\Ga, \faded{x:}\ A \vdash \faded{t :}\ B}
    {\Ga \vdash \faded{\lambda x.\ t :}\ A \to B}
  %

  \infer
    {
      \Ga \vdash \faded{u :}\ A \to B \\
      \Ga \vdash \faded{v :}\ A
    }
    {\Ga \vdash \faded{u\ v :}\ B}
  %

  \infer
    {\faded{(x : }\ A\faded{)} \in \Ga}
    {\Ga \vdash \faded{x :}\ A}
  %
\end{mathpar}

If we forget about the terms for a bit, they are exactly the \emph{same}.
The Curry-Howard isomorphism sets up a parallel between types and propositions
and between programs and proofs.
This is the origin of a very fruitful relationship between the two worlds.

The correspondence does not stop there, computation also plays a part in it.
Consider the term \((\lambda x. x)\ y\) it can be typed as follows
(assuming \(y\) has type \(A\)).
\[
  \infer
    {
      \infer*
        {
          \infer*
            { }
            {y : A, x : A \vdash x : A}
        }
        {y : A \vdash \lambda x. x : A \to A}
      \\
      \infer*
        { }
        {y : A \vdash y : A}
      %
    }
    {y : A \vdash (\lambda x. x)\ y : A}
  %
\]

However, we know it can compute with \(\beta\)-reduction:
\[
  (\lambda x. x)\ y \red_\beta y
\]
This yields a simpler term with a simpler typing derivation.
\[
  \infer
    { }
    {y : A \vdash y : A}
\]
This means that \(\beta\)-reduction allows for simplification of proofs.
This corresponds to the \emph{cut-elimination} I introduced in
\nrefch{proof-theory} which removes all superfluous applications of \emph{cut}
(or \emph{modus ponens} in our case) in the proof.
Here it removes direct applications of a function to an argument.
Thanks to that, a proof of \(A \to B\) is in fact an algorithm turning a proof
of \(A\) into a proof of \(B\).

All this is very useful and was exciting for me. However, simple types are quite
limited~\sidecite{zakrzewski2007definable} and do not allow for a proper
handling of things like quantifiers (\(\forall\) and \(\exists\)).
Hence the need for \emph{polymorphism} and \emph{dependent} types.
The latter will be the subject of the next chapter.