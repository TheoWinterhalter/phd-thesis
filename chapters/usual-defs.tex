% \setchapterpreamble[u]{\margintoc}
\chapter{Usual definitions in type theory}
\labch{usual-defs}

Dependent type theory as presented in \nrefch{dependent-types} is rather barren.
It really shines when extended with some interesting principles and datatypes.
I will give an overview of these features---with the \Coq proof assistant in
mind---and focus mainly on those that are relevant to this thesis.

\section{Inductive types and pattern-matching}
\labsec{inductive-types}

Inductive types are probably the most emblematic feature of dependent type
theory. They can be seen as an extension of the variant datatypes present in
\ocaml like the type of lists.
\marginnote[0.6cm]{
  A list, say of integers, is either empty (\mintinline{ocaml}{nil}) or some
  head \mintinline{ocaml}{h : int} and some tail
  \mintinline{ocaml}{t : int list}, written \mintinline{ocaml}{cons h t}.
}
\begin{minted}{ocaml}
type 'a list =
| nil
| cons of 'a * 'a list
\end{minted}
Here, the \mintinline{ocaml}{'a} represents \emph{any} type. For instance,
\mintinline{ocaml}{int list} is the type of lists of integers.
This is called polymorphism.

They come in different flavours which I will try to explain.

\subsection{Variants}

The simplest case of inductive types is that of variants. They consist in a list
of different options.

\paradot{Booleans}

\(\bool\) is the type inhabited by \(\ttrue\) and \(\ffalse\).
\begin{mathpar}
  \infer
    { }
    {\Ga \vdash \bool}
  %

  \infer
    { }
    {\Ga \vdash \ttrue : \bool}
  %

  \infer
    { }
    {\Ga \vdash \ffalse : \bool}
  %
\end{mathpar}

In \Coq you would write it as follows:
\begin{minted}{coq}
Inductive bool : Type :=
| true
| false.
\end{minted}
In other words, \mintinline{coq}{true} and \mintinline{coq}{false} are the only
\emph{constructors} of the type \mintinline{coq}{bool}.

Of course, having those is not nearly enough without the usual
\(\mathsf{if}\) construct.
For instance \mintinline{coq}{if b then 0 else 1} will return
\mintinline{coq}{0} if \mintinline{coq}{b} is \mintinline{coq}{true}
and \mintinline{coq}{1} if it is \mintinline{coq}{false}.
This is already something that makes sense in the simple
case\sidenote{See \nrefch{simple-types}}, but with dependent types the case
analysis is also dependent on the scrutinee (\ie the boolean \(b\) in that
case). The corresponding typing rule is an \emph{elimination} rule as we have
seen in \nrefch{proof-theory}.
\begin{mathpar}
  \infer
    {
      \Ga \vdash b : \bool \\
      \Ga, x : \bool \vdash P \\
      \Ga \vdash u : P[x \sto \ttrue] \\
      \Ga \vdash v : P[x \sto \ffalse]
    }
    {\Ga \vdash \tif{b}{x.P}{u}{v}}
  %
\end{mathpar}
Before we break down the typing rule, let me show you the computational
behaviour of \(\mathsf{if}\).
\begin{mathpar}
  \begin{array}{lcl}
    \tif{\ttrue}{x.P}{u}{v} &\red& u \\
    \tif{\ffalse}{x.P}{u}{v} &\red& v
  \end{array}
\end{mathpar}
It is still the same as the well-known \(\mathsf{if}\), except that we are more
liberal in the types given to the two branches: they do not have to match as they
can now depend on the boolean. The \(x.P\) notation means that \(P\) lives in a
context extended by \(x\) (of type \(\bool\)).

One can for instance write the following definition where the return type does
\emph{not} depend on the boolean:
\[
  \mathsf{P} := \lambda (b : \bool).\ \tif{b}{x.\Type}{\bool}{\nat}
\]
where \(\nat\) is the type of natural numbers I am going to present later.
Here, we have \(\mathsf{P}\ \ttrue = \bool\) and \(\mathsf{P}\ \ffalse = \nat\).
Using this as a return type, we can write a dependent version of the
\(\mathsf{if}\):
\[
  \tif{b}{x.\ \mathsf{P}\ x}{\ttrue}{0}
\]
The idea is that in the branch where \(b\) is \(\ttrue\), we have to provide
a boolean, here \(\ttrue\), and in the branch where \(b\) is \(\ffalse\), we
have to provide a nautral number, I picked \(0\).

The \(\mathsf{if}\) is actually just a notation for a more generic construction
called \emph{pattern-matching}. \(\tif{b}{x.P}{u}{v}\) is in fact the term
\[
  \pmatch{b}{x.P}{
    \branch{\ttrue}{u} \\
    \branch{\ffalse}{v}
  }
\]
It describes the case analysis by saying which constructor is sent to which
term. If the scrutinee---here \(b\)---\emph{matches} one of the branches on
left-hand side of \(\mto\), the whole expression will reduce to the
corresponding right-hand side.

\paradot{Unit}

The \(\unit\) type is similar to \(\bool\) but has only one constructor written
\(\tunit\).
\begin{mathpar}
  \infer
    { }
    {\Ga \vdash \unit}
  %

  \infer
    { }
    {\Ga \vdash \tunit : \unit}
  %
\end{mathpar}

In \Coq it is defined as:
\begin{minted}{coq}
Inductive unit : Type :=
| tt.
\end{minted}
And the notation mechanism can help use write \mintinline{coq}{tt}
as \mintinline{coq}{()}.
Once again, pattern-matching allows us to inspect a proof of \(\unit\):
\begin{mathpar}
  \infer
    {
      \Ga \vdash u : \unit \\
      \Ga, x:\unit \vdash P \\
      \Ga \vdash v : P[x \sto \tunit]
    }
    {
      \Ga \vdash
      \pmatch{u}{x.P}{
        \branch{\tunit}{v}
      }
      : P[x \sto u]
    }
  %
\end{mathpar}
As all rules pertaining to pattern-matching, it is an elimination rule.
In fact, pattern-matching is the only way---besides application---to express
elimination in \Coq.

This rule might seem a bit useless, but essentially it means that to prove
anything involving a dependency on a term \(u\) of type \(\unit\), like
\(P[x \sto u]\), it suffices to prove it assuming \(u\) is \(\tunit\):
\(P[x \sto \tunit]\).
When the term \(u\) was already \(\tunit\), the \(\mathsf{match}\) can go away:
\[
  \pmatch{\tunit}{x.P}{\branch{\tunit}{v}} \red v
\]

Sometimes, this type is called \(\top\) as in the logical triviality.

\paradot{Empty type}

The empty (or false) type, \(\bot\), is the dual of the unit type. This time it
has no constructors \emph{at all}.
\begin{mathpar}
  \infer
    { }
    {\Ga \vdash \bot}
  %
\end{mathpar}

In \Coq, it is written in a rather queer manner.
\begin{minted}{coq}
Inductive False :=.
\end{minted}

It represents the data that should never exist, so any term of type \(\bot\)
is a \emph{contradiction} with the hypotheses at hand.
Even though it does not have constructors, pattern-matching still makes sense on
such terms.
\marginnote[1.6cm]{
  The pattern-matching does not have any branches, hence the empty space.
}
\begin{mathpar}
  \infer
    {
      \Ga \vdash t : \bot \\
      \Ga, x:\bot \vdash P
    }
    {\Ga \vdash \pmatch{t}{x.P}{} : P[x \sto t]}
  %
\end{mathpar}
This is the essence of the \emph{principle of explosion}:
\emph{ex falso quodlibet}, from falsehood, anything follows.
Here we are able to conjure some inhabitant of \(P[x \sto t]\) from thin air.
The \(P\) is typically not dependent on the proof of \(\bot\), meaning that from
an inhabitant of \(\bot\) we can get an inhabitant of \emph{any} type.

In dependent type theory we will define negation \(\neg P\) as \(P \to \bot\).
As such, we might still end up with inhabitants of \(\bot\) when dealing with
hypotheses of the shape \(\neg P\).

\subsection{Recursive types}

% Inductive types are morally types of trees. For now I only prensented types
% consisting only of leaves. To allow for nodes with subtrees, constructors can
% take subtrees (or subterms) as arguments.
% \todo{Not talk about trees, or later?}
For now, I only presented inductive types that consist in enumerations of cases.
Inductive types can be more complex and constructors can take subterms as
arguments.
The best and simplest example of those is that of natural numbers.

\paradot{Natural numbers}

The way we represent \emph{unary} natural numbers in type theory is by saying
a natural number is either \(0\) or the successor of another natural number
\(n\), \ie \(n + 1\). We usually write the successor operation \(\natsucc\).
So natural numbers are \(0\), \(\natsucc\ 0\), \(\natsucc\ (\natsucc\ 0)\), etc.
respectively representing \(0\), \(1\), \(2\), \dots
\marginnote[2.7cm]{
  Notice how \(u_\natsucc\) is allowed to mention \(m\). The variable is bound
  by the pattern \(\natsucc\ m\) on the left-hand side.
}
\begin{mathpar}
  \infer
    { }
    {\Ga \vdash \nat}
  %

  \infer
    { }
    {\Ga \vdash \zero : \nat}
  %

  \infer
    {\Ga \vdash n : \nat}
    {\Ga \vdash \natsucc\ n : \nat}
  %

  \infer
    {
      \Ga \vdash n : \nat \\
      \Ga, x:\nat \vdash P \\
      \Ga \vdash u_\zero : P[x \sto \zero] \\
      \Ga, m:\nat \vdash u_\natsucc : P[x \sto \natsucc\ m]
    }
    {
      \Ga \vdash
      \pmatch{n}{x.P}{
        \branch{\zero}{u_\zero} \\
        \branch{\natsucc\ m}{u_\natsucc}
      }
      : P[x \sto n]
    }
  %
\end{mathpar}
The first three rules are \emph{introduction} rules, and as before, the
typing rule for pattern-matching is an \emph{elimination} rule.

The rules come together with the computation rules
\[
  \begin{array}{lcl}
    \pmatch{\zero}{x.P}{
      \branch{\zero}{u_\zero} \\
      \branch{\natsucc\ m}{u_\natsucc}
    }
    &\red&
    u_\zero \\
    \pmatch{\natsucc\ n}{x.P}{
      \branch{\zero}{u_\zero} \\
      \branch{\natsucc\ m}{u_\natsucc}
    }
    &\red&
    u_\natsucc[m \sto n]
  \end{array}
\]

As with the previous examples, this allows us to do case analysis on natural
numbers but this is no longer sufficient to effectively reason on them.
The bare minimum we would require is to do induction on natural numbers.

There are two main ways of achieving this.
\begin{itemize}
  \item \emph{Eliminators.} This method consists in assuming the induction
  principle of \(\nat\) directly.
  \[
    \natrec :
      \Pi\ P.\
        P\ \zero \to
        (\Pi n.\ P\ n \to P\ (\natsucc\ n)) \to
        \Pi n. P\ n
  \]
  The type of \(\natrec\) means that, for all proposition \(P\) on natural
  numbers, if \(P\ \zero\) holds and \(P\ n\) implies \(P\ (n + 1)\), then
  \(P\) holds for all natural numbers, which is the well-known induction
  principle on \(\mathbb{N}\).
  The eliminator comes with computation rules as well:
  \[
    \begin{array}{lcl}
      \natrec\ P\ p_\zero\ p_\natsucc\ \zero &\red& p_\zero \\
      \natrec\ P\ p_\zero\ p_\natsucc\ (\natsucc\ n) &\red&
      p_\natsucc\ n\ (\natrec\ P\ p_\zero\ p_\natsucc\ n) \\
    \end{array}
  \]
  They tell us how to produce proofs from an induction. To get a proof of
  \(P\ \zero\) we simply use \(p_\zero : P\ \zero\), and to get a proof of
  \(P\ (n+1)\) we apply \(p_\natsucc\) which produces a proof of \(P\ (n+1)\)
  from a proof of \(P\ n\), the latter is obtained by applying the eliminator
  again.
  There are ways to generate eliminators
  automatically~\sidecite{kovacs2020signatures} and \Coq does it to some extent.
  The troublesome part is getting the right computation rules and dealing with
  more complicated kinds of inductive types such as nested inductive types
  that I will not describe.

  \item \emph{Fixed-points.} The method used in \Coq comes from a combination of
  pattern-matching and a fixed-point operator.
  \marginnote[0.7cm]{
    The notation \(\Pi \D. T\) is to quantify over a whole context.
    If \(\D\) is \(x : A, y : B\), then \(\Pi \D. T\) is
    \(\Pi (x:A)\ (y:B).\ T\).
    When \(\D\) is empty, this is just \(T\).
  }
  \[
    \infer
      {
        \Ga \vdash \Pi \Delta. T \\
        \Ga, f : \Pi \Delta. T, \D \vdash t : T \\
        \highlight{f \vdash_n t \text{ termination checks}}
      }
      {\Ga \vdash \fixp_n (f : \Pi \Delta. T).\ t : \Pi \Delta. T}
    %
  \]
  As you can see there is an extra condition I called `termination checking'.
  If we were not to restrict the definition of fixed-points to terminating
  functions we would end up with an inconsistent theory. Indeed, one could
  inhabit the empty with the following trivial fixed-point which just calls
  itself directly:
  \[
    \fixp (f : \bot).\ f
  \]
  The typing derivation showing it has type \(\bot\) is the following%
  \marginnote[0.9cm]{
    Here, I took \(\D \coloneqq \ctxempty\) and \(T \coloneqq \bot\).
  }%
  \[
    \infer*
      {
        \infer*
          { }
          {\vdash \bot}
        \\
        \infer*
          { }
          {f : \bot \vdash f : \bot}
        %
      }
      {\vdash \fixp (f : \bot).\ f : \bot}
    %
  \]
  The termination checking condition roughly verifies that the \(n\)-th argument
  in \(\D\) (\ie \(x_n\) if \(\D = x_1 : A_1, \dots x_m : A_m\)) is only fed to
  \(f\) (\ie recursive calls) in a decreasing manner.
  I write \(n\) as the subscript to the judgment to indicate that is relevant
  to the termination checking.
  If the \(n\)-th argument is a natural number, it roughly means that every
  recursive call must be done on a smaller natural number.
  This so-called guard condition is a complicated matter and out of scope of
  this thesis. It is probably better studied in
  \sidecite{gimenez1998structural,gimenez1994codifying}.
  In general this can be thought as only subterms of the argument are passed on
  to \(f\) (thus, the fixed-point above is illegal because there is no argument
  that becomes strictly smaller, it is however fine to call \(f\ n\) to compute
  \(f\ (\natsucc\ n)\) as \(n\) is a (strict) subterm of \(\natsucc\ n\)).
  The computational behaviour of the fixed-point operator is via the unfolding
  of its definition.
  \[
    \fixp_n (f : \Pi \D.T).\ t \red
    \lambda \D.\ t[f \sto \fixp_n (f : \Pi \D.T).\ t]
  \]
  Of course, doing this would defeat the purpose of the termination checker:
  ensuring that the obtained definition terminates. As you can see you can
  unfold the fixed-point indefinitely.
  The way we prevent that is by using a syntactical guard on the reduction rule.
  It is instead the following.
  \marginnote[0.6cm]{
    \(\mathbf{C}\) stands for a constuctor like \(\zero\) or \(\natsucc\).
  }
  \[
    \begin{array}{lc}
      (\fixp_n (f : \Pi \D.T).\ t)\
      u_1\ \dots\ u_{n-1}\ (\mathbf{C}\ v_1\ \dots\ v_m)
      &\red \\
      (\lambda \D.\ t[f \sto \fixp_n (f : \Pi \D.T).\ t])\
      u_1\ \dots\ u_{n-1}\ (\mathbf{C}\ v_1\ \dots\ v_m)
    \end{array}
  \]
  A fixed-point can thus only be unfolded when its recursive argument is a
  constructor. It will thus have to consume it and apply the recursive calls
  (\ie the fixed-point itself now that is has been unfolded) on subterms of the
  constructor (typically one of the \(v_i\)).
  We can then write down the induction principle \(\natrec\) with \(\fixp\)
  and \(\mathsf{match}\).
  \marginnote[1.5cm]{
    Notice how \(m\) is a subterm of \(\natsucc\ m\) in the recursive call to
    \(f\). It is indeed its fourth argument.
  }
  \[
    \begin{array}{l}
      \fixp_4\ (f : \Pi\ P\ p_\zero\ p_\natsucc\ n.\ P\ n). \\
      \quad \lambda\ P\ p_\zero\ p_\natsucc\ n. \\
      \quad \pmatch{n}{x.\ P\ x}{
        \branch{\zero}{p_\zero} \\
        \branch
          {\natsucc\ m}
          {p_\natsucc\ m\ (f\ P\ p_\zero\ p_\natsucc\ \highlight{m})}
      }
    \end{array}
  \]
  It will have the same reduction rules as the eliminator shown above.
\end{itemize}

I will not go into that much detail for other inductive types as they will all
follow more or less the same schema.

\subsection{Parameterised inductive types}

Parameterised inductive types, as the name suggests, can take parameters.
In \ocaml, parameters are necessarily types, in \Coq they are often types but
they can be natural numbers or a term of any other type.
For instance, in the type \(\tlist\ A\), \(A\) is a parameter that specifies the
nature of the objects in the list.

\paradot{Lists}

I already presented the list type of \ocaml, and its \Coq counterpart is not
much different.
\begin{mathpar}
  \infer
    {\Ga \vdash A}
    {\Ga \vdash \tlist\ A}
  %

  \infer
    {\Ga \vdash A}
    {\Ga \vdash \nil : \tlist\ A}
  %

  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash h : A \\
      \Ga \vdash t : \tlist\ A
    }
    {\Ga \vdash h :: t : \tlist\ A}
  %

  \infer
    {
      \Ga \vdash l : \tlist\ A \\
      \Ga, x : \tlist\ A \vdash P \\
      \Ga \vdash u_\nil : P[x \sto \nil] \\
      \Ga, h : A, t : \tlist\ A \vdash u_{::} : P[x \sto h :: t]
    }
    {
      \Ga \vdash
      \pmatch{l}{x.P}{
        \branch{\nil}{u_\nil} \\
        \branch{h :: t}{u_{::}}
      }
      : P[x \sto l]
    }
  %
\end{mathpar}
The computation rules are the ones you should expect by now.
Using pattern-matching and fixed-points---as we did for natural numbers---we get
the induction principle on lists of type
\[
  \Pi\ A\ (P : \tlist\ A \to \Type).\
  P\ \nil \to
  (\Pi\ h\ t.\ P\ t \to P\ (h :: t)) \to
  \Pi\ l.\ P\ l
\]

\paradot{Options}

\(\option\ A\) represents a potential data of type \(A\) but it could also not
be present. It is optional.
\begin{mathpar}
  \infer
    {\Ga \vdash A}
    {\Ga \vdash \option\ A}
  %

  \infer
    {\Ga \vdash a : A}
    {\Ga \vdash \some\ a : \option\ A}
  %

  \infer
    {\Ga \vdash A}
    {\Ga \vdash \none : \option\ A}
  %

  \infer
    {
      \Ga \vdash o : \option\ A \\
      \Ga, x : \option\ A \vdash P \\
      \Ga, a : A \vdash u_\some : P[x \sto \some\ a] \\
      \Ga \vdash u_\none : P[x \sto \none]
    }
    {
      \Ga \vdash
      \pmatch{o}{x.P}{
        \branch{\some\ a}{u_\some} \\
        \branch{\none}{u_\none}
      }
      : P[x \sto o]
    }
  %
\end{mathpar}
It gives us an easy way of representing functions that are not defined on their
whole domain (they return \(\none\) when they are not).
For instance we could write a division function \(\mathsf{divide}\) such that
\[
  \mathsf{divide}\ x\ 0 = \none
\]
meaning that division by \(0\) is not defined, while in other cases it would
return \(\some\), for example
\[
  \mathsf{divide}\ 10\ 5 = \some\ 2
\]

\paradot{Sum types}

Simple sums \(A + B\) consist in a disjunction of cases. A proof of \(A + B\)
is either a proof of \(A\) or a proof of \(B\).
\begin{mathpar}
  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash B
    }
    {\Ga \vdash A + B}
  %

  \infer
    {
      \Ga \vdash a : A \\
      \Ga \vdash B
    }
    {\Ga \vdash \inl\ a : A + B}
  %

  \infer
    {
      \Ga \vdash b : B \\
      \Ga \vdash A
    }
    {\Ga \vdash \inr\ b : A + B}
  %

  \infer
    {
      \Ga \vdash p : A + B \\
      \Ga, x : A + B \vdash P \\
      \Ga, a : A \vdash u : P[x \sto \inl\ a] \\
      \Ga, b : B \vdash v : P[x \sto \inr\ b]
    }
    {
      \Ga \vdash
      \pmatch{p}{x.P}{
        \branch{\inl\ a}{u} \\
        \branch{\inr\ b}{v}
      }
      : P[x \sto p]
    }
  %
\end{mathpar}
The rules should be reminiscent of the introduction and elimination rules of
disjunction \(A \vee B\) in \NJ, presented in \nrefch{proof-theory}.

\paradot{Simple products}

Simple products \(A \times B\) are types of pairs of elements, one in \(A\)
and one in \(B\). None too surprising.
\marginnote[0.2cm]{%
  You can once again refer to \nrefch{proof-theory} and find a parallel with the
  rules regarding conjunction \(A \wedge B\) in \NJ.
}%
\begin{mathpar}
  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash B
    }
    {\Ga \vdash A \times B}
  %

  \infer
    {
      \Ga \vdash a : A \\
      \Ga \vdash b : B
    }
    {\Ga \vdash (a,b) : A \times B}
  %

  \infer
    {
      \Ga \vdash p : A \times B \\
      \Ga, x : A \times B \vdash P \\
      \Ga, a : A, b : B \vdash t : P[x \sto (a,b)]
    }
    {
      \Ga \vdash
      \pmatch{p}{x.P}{
        \branch{(a,b)}{t}
      }
      : P[x \sto p]
    }
  %
\end{mathpar}

This presentation of pairs is called \emph{positive} as it uses a constructor.
In the section on records, I will show the \emph{negative} version.

\paradot{\(\Sigma\)-types}

\marginnote[0.2cm]{
  In the simple case we have sums \(A + B\), products \(A \times B\)
  and exponentials \(B^A\) or \(A \to B\). In the dependent case however, it is
  all shifted: we have sums \(\Sigma A.B\) and products \(\Pi A.B\).
}
Dependent sums or \(\Sigma\)-types are a generalisation of simple products to
dependent types. They are the way to represent existential quantifiers
(except in a---usually---constructive way): \(\Sigma (x:A).P\ x\) is proven by
giving a term \(t : A\) and a proof of \(P\ t\).
\begin{mathpar}
  \infer
    {
      \Ga \vdash A \\
      \Ga, x : A \vdash B
    }
    {\Ga \vdash \Sigma (x:A). B}
  %

  \infer
    {
      \Ga, x:A \vdash B \\
      \Ga \vdash a : A \\
      \Ga \vdash b : B[x \sto a]
    }
    {\Ga \vdash \dpair{a,b} : \Sigma (x:A).B}
  %

  \infer
    {
      \Ga \vdash p : \Sigma (x:A).B \\
      \Ga, x : \Sigma (x:A).B \vdash P \\
      \Ga, a : A, b : B[x \sto a] \vdash t : P[x \sto \dpair{a,b}]
    }
    {
      \Ga \vdash
      \pmatch{p}{x.P}{
        \branch{\dpair{a,b}}{t}
      }
      : P[x \sto p]
    }
  %
\end{mathpar}
This captures the computational content of existentials: from a proof of
existence you can extract a witness (the \(a\) in \(\dpair{a,b}\)). This is
another important example of constructivism.

Once more, this is the \emph{positive} presentation of \(\Sigma\)-types.
With this presentation it is also possible to restrict \(P\) in case we do not
want to make it possible to extract the witness.

\marginnote[0.2cm]{
  The \(\_\) is there to note that \(B\) does not depend on the variable in
  \(A\).
}
It is also worth noting that simple products are a particular case of
\(\Sigma\)-type: \(A \times B\) can be defined as \(\Sigma (\_:A).B\).

\subsection{Indexed inductive types}

\marginnote[1cm]{
  In all the examples I showed, the parameters are \emph{uniform}, \ie they are
  the same everywhere, but there can be non uniform occurrences of them where
  the recursive argument is at a different parameter.
}
Inductive types can have parameters, but they can also have \emph{indices}.
They are similar to parameters in that they are arguments to the inductive type
but, while parameters are always the same in the \emph{conclusion} of the type
of a constructor, indices can vary.
I will show the two most talked about cases of indexed inductive types: vectors
and equality.

\paradot{Vectors}

Vectors are length-indexed lists: \(\tvec_A\ n\) is type of lists of length
\(n\) whose elements inhabit \(A\). In this case, \(A\) is a parameter and
\(n\) is an index.
\marginnote[1cm]{
  I highlight the index for the constructors to show how it differs.
  \(\vnil\) is the only list of length \(0\) and a \(\vcons\) always adds one
  element to a vector.
}
\begin{mathpar}
  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash n : \nat
    }
    {\Ga \vdash \tvec_A\ n}
  %

  \infer
    {\Ga \vdash A}
    {\Ga \vdash \vnil : \tvec_A\ \highlight{\zero}}
  %

  \infer
    {
      \Ga \vdash a : A \\
      \Ga \vdash n : \nat \\
      \Ga \vdash v : \tvec_A\ n
    }
    {\Ga \vdash \vcons\ a\ n\ v : \tvec_A\ \highlight{\natsucc\ n}}
  %

  \infer
    {
      \Ga \vdash v : \tvec_A\ n \\
      \Ga, p : \nat, x : \tvec_A\ p \vdash P \\
      \Ga \vdash u_\vnil : P[p \sto 0, x \sto \vnil] \\
      \Ga, a : A, m : \nat, w : \tvec_A\ m \vdash
      u_\vcons : P[p \sto \natsucc\ m, x \sto \vcons\ a\ m\ w]
    }
    {
      \Ga \vdash
      \pmatch{v}{p.x.P}{
        \branch{\vnil}{u_\vnil} \\
        \branch{\vcons\ a\ m\ w}{u_\vcons}
      }
      : P[p \sto n, x \sto v]
    }
  %
\end{mathpar}
The pattern-matching this time binds \emph{two} variables in the return
predicate: the variable representing the matched term as usual, but also the
index (\ie the length of the vector)! This is because it varies depending on the
branch.

Vectors are pretty useful because, since you account for the length, you can
write safer and more precise functions. For instance, the tail function on lists
would land in an option type (in the case the list is \(\nil\)), but here we
can easily say it should only take some \(\tvec_A\ (\natsucc\ n)\) as argument,
ruling out the empty vector completely.
\marginnote[1.4cm]{
  The return type is also more precise: the tail of a list of length
  \(\natsucc\ n\) is of length \(n\).
}
\[
  \begin{array}{l}
    \lambda\ A\ (n : \nat)\ (v : \tvec_A\ (\natsucc\ n)). \\
    \pmatch{v}{p.x.\ \tvec_A\ n}{
      \branch{\vcons\ a\ m\ w}{w}
    }
  \end{array}
\]
Notice how I did not even provide a branch for \(\vnil\) because it will always
by ill-typed.%
\marginnote[-0.1cm]{%
  By vanilla \Coq I mean \Coq without anything extra like plugins.
}%
This is unfortunately impossible in vanilla \Coq but is supported
in \Agda natively and can be achieved in \Coq with \Program or the \Equations
plugin~\sidecite{DBLP:conf/itp/Sozeau10,sozeau2019equations}.
With the typing rules I gave it is still possible to do it, it is just that we
have to conclude using the elimination of \(\bot\) in the \(\vnil\) branch.

\paradot{Equality}
As equality is special I will only give a brief definition before we discuss it
in more detail later in this chapter. In \nrefch{flavours} I will also show
different notions of equality.
%
We already have one notion of equality in conversion, sometimes called
definitional equality. This notion is external in type theory and one cannot
reason about conversion. As such we want a notion of equality on which we can
reason, one which is internal to type theory. We talk this time of
\emph{propositional equality}.
%
We want the type \(u =_A v\) to represent equality of terms \(u\) and \(v\) of
type \(A\). When can you prove that \(u\) and \(v\) are \emph{equal}?
When they are the same!%
\marginnote[0.1cm]{
  This notion of `being the same', is conversion: if \(u\) and \(v\) are
  convertible then \(\refl{A} u\) is a proof of \(u =_A v\).
  This comes from the fact that, if \(u \equiv v\), we have
  \(u =_A u \equiv u =_A v\).
}%
\begin{mathpar}
  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash u : A \\
      \Ga \vdash v : A
    }
    {\Ga \vdash u =_A v}
  %

  \infer
    {\Ga \vdash u : A}
    {\Ga \vdash \refl{A} u : u =_A u}
  %

  \infer
    {
      \Ga \vdash u : A \\
      \Ga \vdash v : A \\
      \Ga \vdash e : u =_A v \\
      \Ga, x : A, p : u =_A x \vdash P \\
      \Ga \vdash t : P[x \sto u, p \sto \refl{A} u]
    }
    {
      \Ga \vdash
      \pmatch{e}{x.p.P}{
        \branch{\refl{}}{t}
      }
      : P[x \sto v, p \sto e]
    }
  %
\end{mathpar}
\(\refl{A} u\) is the reflexivity proof and it \emph{unifies} \(u\) and \(v\)
in its return type. Hence you can conclude that \(v\) is an index while \(A\)
and \(u\) are parameters.
The pattern-matching in the case where \(P\) does not depend on the equality
\(p\) helps us recover Leibniz's principle stating that \(u = v\) means that
for every \(P\), \(P\ x \to P\ y\).
In particular this allows us to \emph{rewrite} along equalities, \ie changing
something for something else equal to it in an expression.

Generally speaking indices are complex to deal with and it is related to how
equality is complex in type theory, hence the multiple approaches there are to
it.

\subsection{Other inductive types}

There are other kinds of inductive types and I shall go briefly over some of
them.

\paradot{Mutual inductive types}

Sometimes you want to define two notions at the same time because they are
linked or interleaved.
Take the notion of odd and even numbers for instance. They can both be defined
independently or one built on top of the other, but they can also be defined
mutually: \(0\) is even, when \(n\) is even, \(n+1\) is odd and when \(n\)
is odd, \(n+1\) is even.
In \Coq it goes like this.
\begin{minted}{coq}
Inductive even : nat -> Type :=
| even_O : even 0
| even_S : forall n, odd n -> even (S n)

with odd : nat -> Type :=
| odd_S : forall n, even n -> odd (S n).
\end{minted}

To deal with them you need mutual fixed-points.

\paradot{Inductive inductive types and induction recursion}

\marginnote[0.7cm]{
  In the \emph{mutual} case, only the constructors could mention the other
  inductive types.
}
Pushing even further in that direction, come inductive inductive
types~\sidecite[1.3cm]{forsberg2010inductive} where the type of inductive types
can also depend on the other inductive types.
Induction recursion~\sidecite[1.2cm]{dybjer2000general} allows you to define
inductive types mutually with functions acting on them.

Both these features are not available in \Coq yet, but are present in \Agda.

\section{Coinductive types and records}

Not all data is best represented inductively, instead of constructors it is
possible to talk about \emph{destructors}, \ie observations (in other words we
are more interested in what to do with the data rather than how to build it).

\subsection{Records}

Records are datatypes containing different fields. In \Coq they can be defined
like this.
\begin{minted}{coq}
Record prod A B := pair {
  fst : A ;
  snd : B
}
\end{minted}
This corresponds to another way of defining \(A \times B\).

Historically in \Coq, records are defined as inductive types with one
constructor. The above definition is in fact syntactic sugar for
\begin{minted}{coq}
Inductive prod A B :=
| pair : A -> B -> prod A B.

Definition fst A B (p : prod A B) : A :=
  match p with
  | pair a b => a
  end.

Definition snd A B (p : prod A B) : B :=
  match p with
  | pair a b => b
  end.
\end{minted}
This is what we call \emph{positive} records, corresponding to the presentation
of \(A \times B\) and \(\Sigma (x:A).B\) I gave earlier.

There is however an option in \Coq to instead use a \emph{negative}
presentation:
\begin{minted}{coq}
Set Primitive Projections.
\end{minted}
Once it is set the definition above of the record becomes primitive.
The constructor \mintinline{coq}{pair} is the one that becomes a definition:
\begin{minted}{coq}
Definition pair A B (a : A) (b : B) : prod A B :=
  {|
    fst := a ;
    snd := b
  |}.
\end{minted}
The data is accessed using the projections \mintinline{coq}{fst} and
\mintinline{coq}{snd}. They are \emph{destructors} in that they correspond to
ways to observe pairs, by opposition to the construtors of pairs of the positive
version: \mintinline{coq}{pair}.

I will now give the typing rules of negative \(\Sigma\)-types.
\begin{mathpar}
  \infer
    {
      \Ga \vdash A \\
      \Ga, x : A \vdash B
    }
    {\Ga \vdash \Sigma (x:A). B}
  %

  \infer
    {\Ga \vdash p : \Sigma (x:A).B}
    {\Ga \vdash \pi_1\ p : A}
  %

  \infer
    {\Ga \vdash p : \Sigma (x:A).B}
    {\Ga \vdash \pi_2\ p : B[x \sto \pi_1\ p]}
  %

  \infer
    {
      \Ga \vdash a : A \\
      \Ga, x:A \vdash B \\
      \Ga \vdash b : B[x \sto a]
    }
    {\Ga \vdash \dpair{a,b} : \Sigma (x:A).B}
  %
\end{mathpar}

The way to think about them is that when providing \(\dpair{a,b}\) you are
actually saying how the term will behave when projected (by either \(\pi_1\)
and \(\pi_2\)).
This is illustrated by the computation rules.
\[
  \begin{array}{lcl}
    \pi_1\ \dpair{a,b} &\red& a \\
    \pi_2\ \dpair{a,b} &\red& b
  \end{array}
\]

I already hinted at this, but with this presentation it is not possible to
restrict usage of \(\pi_1\), it is always possible to recover the witness.

\subsection{Coinductive types}

Coinductive types are the dual of inductive types and are used to represent
potentially infinite data. Streams of data are one such example: they are
infinite lists, like lists they have heads and tails, except there always is a
tail.
\begin{mathpar}
  \infer
    {\Ga \vdash A}
    {\Ga \vdash \stream\ A}
  %

  \infer
    {\Ga \vdash s : \stream\ A}
    {\Ga \vdash \head\ s : A}
  %

  \infer
    {\Ga \vdash s : \stream\ A}
    {\Ga \vdash \tail\ s : \stream\ A}
  %
\end{mathpar}
In the definition of the coinductive type of streams we define destructors:
\(\head\) and \(\tail\) are means to inspect a \(\stream\).
We cannot use constructors to describe a stream however, as it is infinite.
There are several ways to build such data but the one I find most convincing and
beautiful is to use a dual approach to pattern-matching:
\emph{copattern-matching}~\sidecite{abel2013copatterns}, usually in combination
with co-fixed-point.
In \Agda, the stream consisting only of zeroes can be defined as follows:
\begin{minted}{agda}
zeroes : Stream Nat
head zeroes = zero
tail zeroes = zeroes
\end{minted}
while the stream of natural numbers starting at \(n\) is
\begin{minted}{agda}
seq : Nat → Stream Nat
head (seq n) = n
tail (seq n) = seq (succ n)
\end{minted}
In will not go into further detail about those, coinductive types are not really
well-behaved in \Coq so we mostly ignore them for the time being.
\todo{Matthieu's remark}

\section{Equality}
\labsec{equality-def}

I will here describe some usual definition and concepts associated with equality
in type theory, mostly from the point of view of \Coq.
Again, several notions of equality are of interest and I study them briefly
in \nrefch{flavours}.

\subsection{Basic properties of equality}

\reminder[-0.7cm]{Equality}{
  Equality is given by the following rules.
  \[
    \infer
      {
        \Ga \vdash A \\
        \Ga \vdash u : A \\
        \Ga \vdash v : A
      }
      {\Ga \vdash u =_A v}
    %
  \]
  \[
    \infer
      {\Ga \vdash u : A}
      {\Ga \vdash \refl{A} u : u =_A u}
    %
  \]
  \[
    \infer
      {
        \Ga \vdash u : A \\
        \Ga \vdash v : A \\
        \Ga \vdash e : u =_A v \\
        \Ga, x : A, p : u =_A x \vdash P \\
        \Ga \vdash t : P[x \sto u, p \sto \refl{A} u]
      }
      {
        \Ga \vdash
        \fitmatch{e}{x.p.P}{
          \fitbranch{\mathsf{refl}}{t}
        }
        : P[x \sto v, p \sto e]
      }
    %
  \]
}
Equality, by all means, should be a congruence, and it is. It might be
surprising given that we \emph{only} ask for it to be reflexive.
Induction helps us recover these properties.

We can build a term \(\eqrec\) corresponding to the eliminator for equality.
\[
  \begin{array}{lcl}
    \eqrec &:&
    \Pi\ A\ (u : A)\ (P : \Pi\ (x : A).\ u =_A x \to \Type).\ \\
    && \quad P\ u\ (\refl{A}\ u) \to \\
    && \quad \Pi\ v\ (e : u =_A v).\ P\ v\ e
  \end{array}
\]
Its definition is
% eq_rect =
% fun (A : Type) (x : A) (P : A -> Type) (f : P x) (y : A) (e : x = y) =>
% match e in (_ = y0) return (P y0) with
% | eq_refl => f
% end
% 	 : forall (A : Type) (x : A) (P : A -> Type),
%        P x -> forall y : A, x = y -> P y
\[
  \begin{array}{l}
    \lambda\ A\ u\ P\ t\ v\ e.\ \\
    \pmatch{e}{x.p.\ P\ x\ p}{
      \branch{\refl{}}{t}
    }
  \end{array}
\]

It is often the case that \(P\) does not mention the equality, in which case we
get the simpler notion of \emph{transport}.
\marginnote[1cm]{
  I changed the order of the arguments a bit compared with \(\eqrec\) because
  I find it more legible this way, and because I can, thanks to less
  dependencies.
}
\[
  \begin{array}{lcl}
    \ntransp &:&
    \Pi\ A\ (P : A \to \Type)\ (u\ v : A).\ \\
    && \quad u =_A v \to \\
    && \quad P\ u \to \\
    && \quad P\ v \\
    &\coloneqq& \lambda\ A\ P\ u\ v\ e\ t.\ \\
    && \pmatch{e}{x.p.\ P\ x}{
          \branch{\refl{}}{t}
        }
  \end{array}
\]

Transport will be a good stepping stone to show that equality is a congruence.
\marginnote[1cm]{
  Notice how
  \[
    \ntransp\ A\ (\lambda x.\ u =_A x)\ u\ v\ e
  \]
  has type
  \[
    u =_A u \to u =_A v
  \]
}
\[
  \begin{array}{lcl}
    \mathsf{sym} &:&
    \Pi\ A\ (u\ v : A).\ \\
    && \quad u =_A v \to \\
    && \quad v =_A u \\
    &\coloneqq& \lambda\ A\ u\ v\ e.\ \\
    && \quad \ntransp\ A\ (\lambda x.\ u =_A x)\ u\ v\ e\ (\refl{A} u) \\
    \\
    \mathsf{trans} &:&
    \Pi\ A\ (u\ v\ w : A).\ \\
    && \quad u =_A v \to \\
    && \quad v =_A w \to \\
    && \quad u =_A w \\
    &\coloneqq& \lambda\ A\ u\ v\ w\ e_1\ e_2.\ \\
    && \quad \ntransp\ A\ (\lambda x.\ u =_A x)\ v\ w\ e_2\ e_1 \\
    \\
    \mathsf{cong} &:&
    \Pi\ A\ B\ (f : A \to B)\ (u\ v : A).\ \\
    && \quad u =_A v \to \\
    && \quad f\ u =_B f\ v \\
    &\coloneqq& \lambda\ A\ B\ f\ u\ v\ e.\ \\
    && \quad \ntransp\ A\ (\lambda x.\ f\ u =_B f\ x)\ u\ v\ e\
    (\refl{B} (f\ u))
  \end{array}
\]

\subsection{Independent principles}

I will now present some usual notions associated with equality. I call them
independent because they are neither provable in nor contradictory to the theory
of \Coq. This will be studied in \arefpart{elim-reflection}.

\paradot{\acrfull{UIP}}

\acrshort{UIP} is a principle saying that all proofs of an equality are
themselves equal.
\[
  \mathsf{uip} : \Pi\ A\ (x\ y : A)\ (e \ e' : x =_A y).\ e = e'
\]
In particular---or equivalently---all proofs of \(x = x\) are equal to the
reflexivity proof, this is Streicher's axiom K:
\[
  \mathsf{K} : \Pi\ A\ (x : A)\ (e : x =_A x).\ e =_{x =_A x} \refl{A} x
\]
This can be very useful to deal with higher-level equalities.
This principle can be assumed in \Coq but is not provable. In \Agda however this
principle holds and has to be deactivated with an
option~\sidecite{cockx2016eliminating}.

\paradot{\Acrfull{funext}}

\Acrshort{funext} states that two functions are equal whenever they are
pointwise equal.
\[
  \begin{array}{lcl}
    \mathsf{funext} &:&
    \Pi\ A\ B\ (f\ g : A \to B).\\
    && \quad (\Pi x.\ f\ x =_B g\ x) \to \\
    && \quad f =_{A \to B} g
  \end{array}
\]

There is also a dependent variant of \acrshort{funext}.
\[
  \begin{array}{lcl}
    \mathsf{funextD} &:&
    \Pi\ A\ (B : A \to \Type)\ (f\ g : \Pi. (x : A).\ B\ x).\\
    && \quad (\Pi x.\ f\ x =_{B\ x} g\ x) \to \\
    && \quad f =_{\Pi (x:A).\ B\ x} g
  \end{array}
\]

This corresponds to the usual understanding of equality on functions.
Some systems support it natively with computational content like
\acrfull{OTT}~\sidecite[-0.8cm]{altenkirch2007observational},
\acrfull{STT}~\sidecite[-0.1cm]{altenkirch2019setoid} or
\acrfull{CubicalTT}~\sidecite[0.2cm]{coquand:cubical,cohen2016cubical}.


\subsection{Heterogenous equality}
\labsubsec{hetero-eq-def}

Sometimes, you want to be able to compare terms of different types. This will
happen typically with dependent types, \eg with vectors \(u : \tvec_A\ (n + m)\)
and \(v : \tvec_A\ (m + n)\). It feels like we should be able to compare them
since their types are equal, unfortunately they are usually not convertible
and as such \(u = v\) would be ill-typed.

In such cases one has to consider \emph{heterogenous} equalities---as opposed
to the regular homogenous equality.
\acrfull{JMeq} is one such notion of heterogenous equality, introduced by
\sidecite{mcbride2000dependently}.
It is pretty similar to the equality type I showed earlier, except it uses an
extra index for a second type.
\begin{mathpar}
  \infer
    {
      \Ga \vdash a : A \\
      \Ga \vdash b : B
    }
    {\Ga \vdash \Heq{A}{a}{B}{b}}
  %

  \infer
    {\Ga \vdash a : A}
    {\Ga \vdash \mathsf{hrefl}_A\ a : \Heq{A}{a}{A}{a}}
  %
\end{mathpar}
As usual, it features pattern-matching. Unfortunately, it does not allow us to
recover the property that \emph{heterogenous} equalities where the two types are
in fact the same like \(\Heq{A}{u}{A}{v}\), imply \emph{homogenous} equality
\(u =_A v\).
This has to be added as an extra axiom that is equivalent to \acrshort{UIP}.

For this reason, I personally prefer the more explicit encoding of heterogenous
equality with \(\Sigma\)-types:
\[ \Heq{T}{t}{U}{u} := \Sum{p:\Eq{}{T}{U}} \Eq{}{\transpo{p}\ t}{u} \]
where we write \(\transpo{p}\) for the map from \(T\) to \(U\), it is a notation
for
\[
  \transpo{p} \coloneqq \ntransp\ \Type\ (\lambda x. x)\ T\ U\ p.
\]
When we have \(\Heq{A}{u}{A}{v}\), it means we have a proof of equality
\(p : A = A\) and a proof of \(\transpo{p}\ u = v\). It becomes apparent that
we need \acrshort{UIP} to rewrite \(p\) to \(\mathsf{refl}\) such that the
second equality becomes \(u = v\) as wanted.

\marginnote[0.15cm]{
  Here the notion of heterogenous can be either of the two I presented as they
  are equivalent.
}
\begin{lemma}
  \lablemma{uip-cong}
  Assuming \acrshort{UIP}, if $\isterm{\Ga}{e}{\Heq{A}{u}{A}{v}}$
  then there exists $p$ such that $\isterm{\Ga}{p}{\Eq{A}{u}{v}}$.
\end{lemma}