% \setchapterpreamble[u]{\margintoc}
\chapter{Syntax and formalisation of type theory}
\labch{formalisation}

An interesting fact of type theory (and perhaps one its main selling points) is
that it is a suitable framework in which to reason about type theory.
That being said, representing type theory in itself isn't entirely
straightforward, and some care must be taken. There are actually several choices
to be made when representing type theory and they are not all equivalent or with
the same pros and cons.
I will detail some of them, spending more time on those I ended up choosing
and will try to motivate my choice.

In this chapter I will refer to work done in conjunction with Andrej Bauer and
Philipp Haselwarter called \ftt~\sidecite{formaltypetheory}.

\section{Representation of syntax}

I will first focus on the syntactical side of type theory.
The first important choice being how to represent variables.

\subsection{How to deal with variables}

When writing programs or expressions with binders on paper or on the computer
we will usually use \emph{names}, identifiers for variables like in
\(\lambda x. \lambda y. x\ y\), \(x\) refers to the variable bound by le
outermost \(\lambda\), while \(y\) referes to the variable bound by the
innermost.
The names aren't fundamental in what the term represents:
\(\lambda z. \lambda w. z\ w\) represents \emph{exactly} the same term.
We call this operation \(\alpha\)-renaming, here I \(\alpha\)-renamed \(x\)
to \(z\) and \(y\) to \(w\). This defines the notion of \(\alpha\)-equality
or \(\alpha\)-equivalence.
\[
  \lambda x. \lambda y. x\ y =_\alpha \lambda z. \lambda w. z\ w
\]
However variable names should only be thought of as an abstraction to represent
such terms and not a part of the syntax in itself.

Thinking in terms of variables name can lead to unpleasant examples where
\(\alpha\)-renaming might become a necessity.
For instance, \(\lambda x. \lambda x. x\) is perfectly valid but is easier to
read when renamed to \(\lambda y. \lambda x. x\). This process is called
\emph{shadowing}, when several variables bear the same name in scope, it is the
innermost that takes precedence. This principle is crucial for compositionality.

Even more problems arise when considering substitutions (after all, that is what
variables are for: to be substituted).
If you consider the term \(t \coloneqq x\ (\lambda x. x)\) we have two
occurrences of the name \(x\) but they do \emph{not} represent the same
variable, the first \(x\) is \emph{free} in the term, while the second is
\emph{bound} by the only \(\lambda\).
Now when substituting \(x\) for term \(u\) in \(t\), one has to be careful not
to replace the bound variable \(x\). The expected result is
\[
  t[x \sto u] = u\ (\lambda x. x)
\]
This used to be called \emph{capture-avoiding} substitutions, but I will call
them substitutions, because the operation yielding \(u\ (\lambda x. u)\)
is utterly rubbish and not deserving of a name.

Several solutions have been proposed to this ``problem'' like nominal
sets~\sidecite[-0.4cm]{pitts2001nominal},
\acrfull{HOAS}~\sidecite[0.3cm]{pfenning1988higher} and de Bruijn indices or
levels~\sidecite[0.6cm]{de1978lambda}.
In think de Bruijn indices are exactly what we want when dealing with
\(\lambda\)-terms as they carry the right amount of information.
The idea is to use natural numbers instead of names to indicate how many binders
to traverse before reaching the one introducing the variable.
\[
  \begin{array}{rcl}
    \lambda x.\ \lambda y.\ \lambda z.\ z
    &\to& \lambda\ \lambda\ \lambda\ \db{0} \\
    \lambda x.\ \lambda y.\ \lambda z.\ y
    &\to& \lambda\ \lambda\ \lambda\ \db{1} \\
    \lambda x.\ \lambda y.\ \lambda z.\ x
    &\to& \lambda\ \lambda\ \lambda\ \db{2}
  \end{array}
\]
In this setting the same variable can be represented in different ways:
\[
  \lambda x.\ x\ (\lambda y.\ x\ y)
\]
becomes
\[
  \lambda\ \db{0}\ (\lambda\ \db{1}\ \db{0})
\]
so that \(x\) is now written \(\db{0}\) and \(\db{1}\) depending on whether it
is referenced under the second \(\lambda\) or not.
The following diagram should make things more explicit.

\begin{center}
  \begin{tikzpicture}[remember picture]
    \node (term) {
      \(\subnode{la}{\(\lambda\)}\ \subnode{va}{\(\db{0}\)}\
      (\subnode{lb}{\(\lambda\)}\ \subnode{vb}{\(\db{1}\)}\
      \subnode{vc}{\(\db{0}\)})\)
    } ;
    \draw[barrow, bend right] (va.north) to (la.north) ;
    \draw[barrow, bend left] (vb.south) to (la.south) ;
    \draw[barrow, bend right] (vc.north) to (lb.north) ;
  \end{tikzpicture}
\end{center}

Using this representation, both \(\lambda x.x\) and \(\lambda y.y\) are written
\[
  \lambda\ \db{0}
\]
so that \(\alpha\)-equality is purely syntactic equality.
\(\alpha\)-renaming is not only a problem for pretty-printing.
This also solves the problem of substitutions potentially capturing free
variables: the term \(t \coloneqq x\ (\lambda x.\ x)\) of before is now
\(\db{n}\ (\lambda\ \db{0})\) where \(n\) is some number which should point to
somewhere in the context (same as the \(x\) it replaces).

Notice however that using de Bruijn indices, weakening---\ie putting a term into
an extended context---will now affect the term itself.
If you consider the following weakening, adding one variable \(z\) in the middle
of the context, doesn't affect the term using names.
\marginnote[1cm]{
  I use variables in the scope to make clear where the new variable is inserted
  in the nameless case.
}
\[
  x : A, y : B \vdash x\ y\ (\lambda u.\ x\ y\ u)
  \leadsto
  x : A, z : C, y : B \vdash x\ y\ (\lambda u.\ x\ y\ u)
\]
In the context of de Bruijn indices this becomes
\[
  A, B \vdash
  \highlight{\db{1}}\ \db{0}\ (\lambda\ \highlight{\db{2}}\ \db{1}\ \db{0})
  \leadsto
  A, C, B \vdash
  \highlight{\db{2}}\ \db{0}\ (\lambda\ \highlight{\db{3}}\ \db{1}\ \db{0})
\]


\subsection{Substitutions}

Another choice that is close to the representation of variable is that of
substitutions. There are several ways to represent a substitution in itself,
but I think the main question is whether to make them \emph{explicit} or not,
\ie part of the syntax or not.

With explicit substitutions \(t[\sigma]\) is a term in itself and things like
evaluation of substitutions come as reduction rules:
\[
  x[\sigma] \red \sigma(x)
\]
The question of how you represent substitutions is more curcial in this case,
and there are several ways to do so, the first being introduced in
\sidecite{abadi1991explicit}.

In \ftt where we formalised syntax of type theory in \Coq using explicit
substitutions we settled on the following constructions (I will give them
using typing rules to make their behaviour explicit):
\begin{mathpar}
  \infer
    {\Ga \vdash u : A}
    {\sbzero{A}{u} : \Ga \to \Ga,A}
  %

  \infer
    {\Ga \vdash A}
    {\sbweak{A} : \Ga \to \Ga, A}
  %

  \infer
    {
      \sigma : \Ga \to \D \\
      \D \vdash A
    }
    {\sbshift{A}{\sigma} : \Ga, A[\sigma] \to \D, A}
  %

  \infer
    {\vdash \Ga}
    {\sbid : \Ga \to \Ga}
  %

  \infer
    {
      \sigma : \Ga \to \D \\
      \theta : \D \to \Xi
    }
    {\theta \circ \sigma : \Ga \to \Xi}
  %

  \infer
    {\vdash \Ga}
    {\sbterminal : \Ga \to \ctxempty}
  %
\end{mathpar}

With computation rules such as
\[
  \begin{array}{rcl}
    \db{0}[\sbzero{A}{u}] &\red& u \\
    (\db{n+1})[\sbzero{A}{u}] &\red& \db{n} \\
    \db{n}[\sbweak{A}] &\red& \db{n+1} \\
    \db{0}[\sbshift{A}{\sigma}] &\red& \db{0} \\
    (\db{n+1})[\sbshift{A}{\sigma}] &\red& \db{n}[\sigma][\sbweak{A[\sigma]}] \\
    &\dots&
  \end{array}
\]

However, I find the other option of having substitutions as a meta-operation,
outside of the syntax, more natural. It also helps in keeping the syntax and
rules to a minimum while turning the substitutions notions above into
definitions and properties.
For instance weakening becomes a lemma and not something postulated with the
constructor \(\sbweak{}\).
I will not make a strong case for either choice however as they both have their
own interest.
Note that in \MetaCoq we go with meta-level substitutions.

\subsection{Annotations}

I already touched on this in \nrefch{models}, especially in conclusion of the
cardinal model of type theory, but I will once again talk about annotations
in a broader setting.

The fist question regards annotations of \(\lambda\)-abstractions with the
domain of the function, that is
\[
  \begin{array}{rcl}
    \lambda (x : A).\ t &\vs& \lambda x.\ t
  \end{array}
\]
This opposes the so-called Churh-style (annotation of the domain) to
Curry-style (no annotation whatsoever).
There is eve a third option of including the codomain \(\lambda (x:A).B.\ t\)
that seems necessary for \acrshort{ETT} according to the cardinal model.
\reminder[-0.7cm]{Uniqueness of type}{
  A type theory has uniqueness of type when
  \(\Ga \vdash t : A\) and \(\Ga \vdash t : B\) imply
  \(A \equiv B\).
}
Forgetting the latter there is already a pivot between Church- and Curry-style
in that plays a determinant role in uniqueness of type.
Indeed one cannot hope to have uniqueness of type given that
\(\lambda x.\ x\) has type \(A \to A\) for any \(A\).
The domain annotation is the minimal information required to get uniqueness of
type for \(\lambda\)-abstractions. This miminal information is equivalent to the
minimal information required for infering the type.
This information is minimal in the sense that any more annotations can be
recovered from a theory without them.

\subsection{Universes and types}
\labsubsec{univ-and-types}

Another point of choice is whether to separate types and terms, putting them in
different syntactic classes. One of the great advantages of dependent type
theory over simple type theory is that types and terms can be expressed using
the same language. Keeping them separate might also make sense.

In case we want to keep them distinct, we have to duplicate concepts a bit:
substitution, weakening, and even typing: we have judgments \(\Ga \vdash A\)
for types and \(\Ga \vdash t : A\) for terms.
One of the advantages of this, is that no universes are required to say that
\eg \(\Pi\)-types are in the theory:
\begin{mathpar}
  \infer
    {
      \Ga \vdash A \\
      \Ga, A \vdash B
    }
    {\Ga \vdash \Pi\ A\ B}
  %
\end{mathpar}

This difference become clearer when considering the two main notion of
universes: namely Tarski and Russel universes.

\paradot{Tarski universes}

I already gave a brief presentation of Tarski universes when looking at
categorical models of type theory in \nrefch{models}.
A Tarski universe is a type of \emph{codes} of types, \ie they are terms
representing types. They can de decoded to types using the \(\CEl\) (element)
type constructor.
\begin{mathpar}
  \infer
    {\vdash \Ga}
    {\Ga \vdash \CU}
  %

  \infer
    {\Ga \vdash a : \CU}
    {\Ga \vdash \CEl\ a}
  %
\end{mathpar}

We then need a code for each type we want in this universe (otherwise we only
have variables). For instance here are the codes for \(\Pi\)-types.
\begin{mathpar}
  \infer
    {
      \Ga \vdash a : \CU \\
      \Ga \vdash b : \CEl\ a \to \CU
    }
    {\Ga \vdash \pi(a,b) : \CU}
  %
\end{mathpar}
To relate them to actual \(\Pi\)-types, we simply add computation rules to
\(\CEl\).
\[
  \CEl(\pi(a,b)) \red \Pi (x : \CEl\ a).\ \CEl\ (b\ x)
\]
Arguably we don't even need that, and to avoid duplication we could simply keep
the \(\CEl\)s everywhere
\marginnote[1cm]{
  The \(\lambda (\highlight{x:a}).\ t\) is not a typo. I enfore here \(\lambda\)
  to use codes in their syntax since one should not be able to abstract over
  a type that isn't some \(\CEl\ a\).
}
\begin{mathpar}
  \infer
    {
      \Ga \vdash a : \CU \\
      \Ga, x : \CEl\ a \vdash b : \CU \\
      \Ga, x : \CEl\ a \vdash t : \CEl\ (b\ x)
    }
    {\Ga \vdash \lambda (x:a).\ t : \CEl\ (\pi(a,b))}
  %

  \infer
    {
      \Ga \vdash a : \CU \\
      \Ga, x : \CEl\ a \vdash b : \CU \\
      \Ga \vdash f : \CEl\ (\pi(a,b)) \\
      \Ga \vdash u : \CEl\ a
    }
    {\Ga \vdash f\ u : \CEl\ (b\ u)}
  %
\end{mathpar}
I still prefer having a notion of \(\Pi\)-types that is independent of universes
and codes so that their behaviour is defined once and for all for every
universe.

\paradot{Russel universes}

The other option, the one used in \Coq, \Agda and most type-theory-based proof
assistants, is to use Russel universes. In this setting types are just terms
that inhabit a universe:
\[
  \Ga \vdash A : \Type
\]
As such we don't need something like \(\CEl\) to bridge between the two worlds.
In a sense, this is a special case of Tarki universes where codes are types
and \(\CEl\) is the identity function.

With Russel universes \(\Pi\)-types are typed as follows:
\begin{mathpar}
  \infer
    {
      \Ga \vdash A : \Type \\
      \Ga, A \vdash B : \Type
    }
    {\Ga \vdash \Pi\ A\ B : \Type}
  %
\end{mathpar}

When formalising universes there is a trade-off between the concise Russel
universes and the compartmentalised Tarski universes. On the interface with a
user however, I think universes are best when they are the least invasive, which
works better in a Russel setting. This doesn't prevent the system from having
Tarski universes under the hood.

\subsection{Formalisation of syntax}

Now that I have laid out a few design choices, I will talk a bit about how to
formalise the syntax of a type theory.
As I already mentioned I worked with two such representations, the one in
\MetaCoq and the one in \ftt, both in \Coq.

In \MetaCoq we use an inductive type to represent terms of which I give an
excerpt below:
\begin{minted}{coq}
Inductive term :=
| tRel (n : nat)
| tSort (u : Universe.t)
| tProd (na : name) (A B : term)
| tLambda (na : name) (A t : term)
| tLetIn (na : name) (b B t : term)
| tApp (u v : term)
| tConst (k : kername) (ui : Instance.t)
| tInd (ind : inductive) (ui : Instance.t)
| tConstruct (ind : inductive) (n : nat) (ui : Instance.t)
| tCase
    (indn : inductive * nat)
    (p c : term)
    (brs : list (nat * term))
| tProj (p : projection) (c : term)
| tFix (mfix : mfixpoint term) (idx : nat).
\end{minted}

\begin{itemize}
  \item \mintinline{coq}{tRel n} represents the \(n\)-th de Bruijn index
  \(\db{n}\);
  \item \mintinline{coq}{tSort u} is a universe, as described in the
  \mintinline{coq}{Universe} module;
  \item \mintinline{coq}{tProd na A B} represents a \(\Pi\)-type, herein
  \mintinline{coq}{na} is a \mintinline{coq}{name}: even though we use de Bruijn
  indices, we still have naming annotations for pretty-printing that are
  irrelevant for typing;
  \item \mintinline{coq}{tLambda na A t} represents a \(\lambda\)-abstraction;
  \item \mintinline{coq}{tLetIn na b B t} is a let-binding;
  \item \mintinline{coq}{tApp u v} is application of \mintinline{coq}{u} to
  \mintinline{coq}{v};
  \item the other constructors deal with constants, inductive types and their
  constructors, pattern-matching and fixed-points.
\end{itemize}

In \MetaCoq we try to stick as close as possible to the \Coq implementation,
whereas in \ftt we used a different approach, trying to be modular on the
syntax. Instead of using an inductive type, we use a record with fields for the
types of types, terms, etc. as well as the different syntactic constructs.
Here is also an excerpt of it.
\begin{minted}{coq}
Record Syntax := {
  context      : Type ;
  type         : Type ;
  term         : Type ;
  substitution : Type ;

  ctxempty  : context ;
  ctxextend : context -> type -> context ;

  Prod  : type -> type -> type ;
  Subst : type -> substitution -> type ;
  Uni   : level -> type ;
  El    : level -> term -> type ;

  var     : nat -> term ;
  lam     : type -> type -> term -> term ;
  app     : term -> type -> type -> term -> term ;
  subst   : term -> substitution -> term ;
  uniProd : level -> level -> term -> term -> term ;

  sbzero     : type -> term -> substitution ;
  sbweak     : type -> substitution ;
  sbshift    : type -> substitution -> substitution ;
  sbid       : substitution ;
  sbcomp     : substitution -> substitution -> substitution ;
  sbterminal : substitution
}.
\end{minted}
We also use de Bruijn indices, but most other choices I presented above aren't
made. It may seem like we use Tarski universes, separate terms and types, use
explicit substitutions and fully-annotated terms.
However, these aren't enforced because these aren't \emph{constructors}.
Nothing prevents us from providing an instance where \mintinline{coq}{term}
and \mintinline{coq}{type} are the same and \mintinline{coq}{El} is the identity
(hence Russel universes), functions that ignore the annotations for
\mintinline{coq}{app} and \mintinline{coq}{lam}, or meta-level functions for
the substitutions.
For instance, here is a version with explicit substitutions, but the rest as I
mentioned:
\begin{minted}{coq}
Inductive context : Type :=
| ctxempty : context
| ctxextend : context -> term -> context

with term : Type :=
(* Types *)
| Prod : term -> term -> term
| Uni : syntax.level -> term
(* Terms *)
| var : nat -> term
| lam : term -> term -> term
| app : term -> term -> term
| subst : term -> substitution -> term

with substitution : Type :=
| sbzero : term -> term -> substitution
| sbweak : term -> substitution
| sbshift : term -> substitution -> substitution
| sbid : substitution
| sbcomp : substitution -> substitution -> substitution
| sbterminal : substitution
.

Definition S : Syntax := {|
  context      := context ;
  type         := term ;
  term         := term ;
  substitution := substitution ;

  ctxempty  := ctxempty ;
  ctxextend := ctxextend ;

  Prod   := Prod ;
  Subst  := subst ;
  Uni    := Uni ;
  El i T := T ;

  var n           := var n ;
  lam A B t       := lam A t ;
  app u A B v     := app u v ;
  subst u sbs     := subst u sbs ;
  uniProd i j A B := Prod A B ;

  sbzero     := sbzero ;
  sbweak     := sbweak ;
  sbshift    := sbshift ;
  sbid       := sbid ;
  sbcomp     := sbcomp ;
  sbterminal := sbterminal
|}.

\end{minted}

There is however an important consequence of such a presentation: the syntax is
no longer inductive, so there is a priori no injectivity of constructors and
we might well define all those types to be \mintinline{coq}{unit}, identifying
all those constructs.
This can make it hard to reason about the syntax, and a lot of properties simply
do not hold because you cannot distinguish two different constructs.
As such I am not sure this is the best way to go.
In the remainder of this thesis, I will focus more on approached like that of
\MetaCoq.

\section{Representation of typing}

Now that we have a notion of syntax in mind, we can move on to representing
typing derivations---as well as the notion of conversion.
Once again, we are faced with many choices.

\subsection{Paranoia of typing rules}

If you have a look at the typing rule of the reflexivity constructor of
equality:
\begin{mathpar}
  \infer
    {\Ga \vdash u : A}
    {\Ga \vdash \refl{A} u : u =_A u}
  %
\end{mathpar}
you can see that we require \(u\) to have type \(A\) without asking for
\(A\) to be a type itself.
So perhaps we should consider the following rule instead:
\begin{mathpar}
  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash u : A
    }
    {\Ga \vdash \refl{A} u : u =_A u}
  %
\end{mathpar}
This time, the question can be whether the context \(\Ga\) itself makes sense.
\begin{mathpar}
  \infer
    {
      \vdash \Ga \\
      \Ga \vdash A \\
      \Ga \vdash u : A
    }
    {\Ga \vdash \refl{A} u : u =_A u}
  %
\end{mathpar}

With Andrej Bauer and Philipp Haselwarter in~\sidecite{formaltypetheory} we
desribe the two ends of this spectrum as either \emph{economic} or
\emph{paranoid} versions of the same typing rule.
There is a real question as to which should be chosen. When building derivations
it is nicer to be in an economic setting to avoid big and redundant derivations,
however when building something \emph{from} a derivation, the paranoid version
gives more hyotheses to work on.
Leaving aside the practical side, there is of course the question of whether
they are equivalent or not.

In \ftt we formalise that paranoid and economic type theories are equivalent
provided rules pertaining to variables and constants require that the context
is well-formed even in the economic version:
\[
  \infer
    {
      \vdash \Ga \\
      (x : A) \in \Ga
    }
    {\Ga \vdash x : A}
  %
\]
The proof relies on what we called sanity properties (and that I called validity
in \nrefch{desirable-props}): when \(\Ga \vdash t : A\) then \(\Ga \vdash A\)
and when \(\Ga \vdash A\) then \(\vdash \Ga\).

\marginnote[0.2cm]{
  Note that this approach is assuming you don't really care about the
  derivations that are produced. If you want to compute on them, having big
  derivations can be a problem and as such it might be worth it to put in the
  extra work to show them equivalent to their paranoid versions while keeping
  the data as small as possible.
}
The right approach to this I think is to put as many information required to
make the proof of sanity/validity \emph{easy} and then use this proof to prove
that the economic versions are admissible.
Empirically it seems to me that you put everything that is needed to do
type inference on the term.
To infer the type of \(\refl{A} u\) you need to check \(u\) has type \(A\),
which usually relies on the well-typedness of \(A\), meaning I would use the
second rule:
\begin{mathpar}
  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash u : A
    }
    {\Ga \vdash \refl{A} u : u =_A u}
  %
\end{mathpar}
It's also easy to see how proving \(\Ga \vdash u =_A u\) will be direct from the
hypotheses.

\subsection{Conversion}

\marginnote[0.5cm]{
  More recently it also had to be modified to include marks to deal with
  definitionally proof-irrelevant universe \mintinline{coq}{SProp}.
}
Conversion can also come in different flavours. In \Coq, conversion is derived
from reduction (as a congruence closure) but is also extended to take
\(\eta\)-expansion into account.
\begin{mathpar}
  \infer
    {
      u \red w \\
      w \equiv v
    }
    {u \equiv v}
  %

  \infer
    {
      u \equiv w \\
      v \red w
    }
    {u \equiv v}
  %

  \infer
    {u =_{\alpha\eta} v}
    {u \equiv v}
  %
\end{mathpar}
Since it's untyped, we have to make sure the type is well-formed in the
conversion rule:
\[
  \infer
    {
      \Ga \vdash t : A \\
      A \equiv B \\
      \Ga \vdash B : \Type
    }
    {\Ga \vdash t : B}
  %
\]

In \Agda on the other hand, conversion is \emph{typed}, that is it is a judgment
of the form
\[
  \Ga \vdash u \equiv v : A
\]
This has a serious advantage in that it allows for rules like
\[
  \infer
    { }
    {\Ga \vdash u \equiv v : \unit}
  %
\]
stating that two terms of the \(\unit\) type are convertible which is not
feasible in an untyped setting. In general it simplifies all type-based
conversion rules. As you can see with the rule above,
\(\Ga \vdash u \equiv v : A\) needn't imply \(\Ga \vdash u : A\) or
\(\Ga \vdash v : A\). It could however, it is again a design choice.
If we do not require it then the conversion rule is similar
\[
  \infer
    {
      \Ga \vdash t : A \\
      \Ga \vdash A \equiv B : \Type \\
      \Ga \vdash B : \Type
    }
    {\Ga \vdash t : B}
  %
\]

Finally, there is a third notion of explicit conversion which stores the
conversion proof in the term:
\[
  \infer
    {
      \Ga \vdash t : A \\
      \Ga \vdash H : A \equiv B \\
      \Ga \vdash B
    }
    {\Ga \vdash t^H : B}
  %
\]

All three notions of conversion are proven equivalent in
\sidecite{van2013explicit}.

\subsection{How intricate need the concepts be?}

If we go back to the notion of typed equality, I mentioned we could enfore
typedness by having rules more like
\[
  \infer
    {
      \Ga \vdash u : \unit \\
      \Ga \vdash v : \unit
    }
    {\Ga \vdash u \equiv v : \unit}
  %
\]
There is a drawback to this however: conversion now mentions typing and typing
mentions conversion, they have to be mutually defined.

In \Coq this would look like
\begin{minted}{coq}
Inductive typing (Γ : context) : term -> term -> Type :=
| typing_lam :
    forall Γ t A B,
      typing (Γ, A) t B ->
      typing Γ (tLambda A t) (tProd A B)

(* ... *)

| typing_conv :
    forall Γ t A B s,
      typing Γ t A ->
      conv Γ A B (tSort s) ->
      typing Γ t B

with conv (Γ : context) : term -> term -> term -> Type :=
| conv_unit :
    forall Γ u v,
      typing Γ u tUnit ->
      typing Γ v tUnit ->
      conv Γ u v tUnit

(* ... *).
\end{minted}
While in \MetaCoq where conversion is untyped, we first define conversion
and then typing on top of it.

We can push things even further and define syntax at the same time as typing
as well. This is called well-typed syntax, and is presented
in~\sidecite{altenkirch2016type} using \acrshortpl{QIT}.
The idea is define contexts, types, terms and substitutions (very similar to
categorical models, see \nrefch{models}) mutually, in a typed way:
\begin{minted}{agda}
data Con : Set
data Ty  : Con → Set
data Tms : Con → Con  → Set
data Tm  : ∀ Γ → Ty Γ → Set
\end{minted}
Contexts are defined with the empty context and the extension as usual, except
now they are well-formed by construction.
\begin{minted}{agda}
data Con where
  •   : Con
  _,_ : (Γ : Con) → Ty Γ → Con
\end{minted}
For types we will stick to \(\Pi\)-types, but we will also have a constructor
for (explicit) substitutions.
\begin{minted}{agda}
data Ty where
  _[_]T : ∀ {Γ Δ} → Ty Δ → Tms Γ Δ → Ty Γ
  Π     : ∀ {Γ} → (A : Ty Γ) (B : Ty (Γ, A)) → Ty Γ
\end{minted}
Substitutions which are given like so.
\begin{minted}{agda}
data Tms where
  ε   : ∀ {Γ} → Tms Γ •
  _,_ : ∀ {Γ Δ A} → (δ : Tms Γ Δ) → Tm Γ (A[δ]T) → Tms Γ (Δ, A)
  id  : ∀ {Γ} → Tms Γ Γ
  _∘_ : ∀ {Γ Δ Ξ} → Tms Δ Ξ → Tms Γ Δ → Tms Γ Ξ
  π₁  : ∀ {Γ Δ A} → Tms Γ (Δ, A) → Tms Γ Δ
\end{minted}
Finally for terms we have something like this, pretty standard except the
application is not the usual\sidenote{
  In fact I wouldn't call it application at
  all as it is not application. It's a key element to getting it however, usual
  application of \mintinline{agda}{(u : Tm Γ (Π A B))} and
  \mintinline{agda}{(v : Tm Γ A)} is given by
  \mintinline{agda}{(app u)[id, u]t}.
} and is rather the inverse operation to
\(\lambda\)-abstraction.
\begin{minted}{agda}
data Tm where
  _[_]t : ∀ {Γ Δ A} → Tm Δ A → (δ : Tms Γ Δ) → Tm Γ (A[δ]T)
  π₂    : ∀ {Γ Δ A} → Tms Γ (Δ, A) → Tm Γ (A[π₁ δ]T)
  app   : ∀ {Γ A B} → Tm Γ (Π A B) → Tm (Γ, A) B
  lam   : ∀ {Γ A B} → Tm (Γ, A) B → Tm Γ (Π A B)
\end{minted}

Now for this to work we need to have conversion rules as well. There are two
ways to do it. The first, possible in \Agda, is to also define those mutually
with conversion and then rely on explicit conversion:
\begin{minted}{agda}
data ConvTy : (Γ : Con) (A B : Ty Γ) → Set
data ConvTm : (Γ : Con) (A : Ty Γ) (u v : Tm Γ A) → Set

-- ...

data Tm where
  -- ...
  coe : ∀ {Γ A B} → ConvTy Γ A B → Tm Γ A → Tm Γ B
\end{minted}
The conversion expressed this way is really tedious as we have to deal with
\mintinline{agda}{coe} itself in the conversion, as well as extend conversion to
context besides the usual type and term conversion.

The solution with \acrlongpl{QIT}---as the name suggests---is instead to
quotient the syntax by the equalities it should verify.
For types those would be things like
\marginnote[0.6cm]{
  Note that \mintinline{agda}{≡} corresponds to the propositional equality of
  \Agda.
}
\begin{minted}{agda}
  [id]T : ∀ {Γ A} → A[id]T ≡ A
  [][]T : ∀ {Γ Δ Ξ A} → {σ : Tms Γ Δ} {δ : Tms Δ Ξ} →
            (A[δ]T)[σ]T ≡ A[δ ∘ σ]T
\end{minted}
and for terms we would have, among other things, the laws for \(\beta\)- and
\(\eta\)-equality.
\begin{minted}{agda}
  Πβ : ∀ {Γ A B} → {t : Tm (Γ, A) B}  → app (lam t) ≡ t
  Πη : ∀ {Γ A B} → {t : Tm Γ (Π A B)} → lam (app t) ≡ t
\end{minted}
Congruence laws simply come from the fact that equality is already a congruence.
Note that \acrshortpl{QIT} are not yet available in \Agda.

I think however that well-typed \emph{syntax} is a misnomer because the data we
are constructing represents derivations rather than terms.
It is very interesting but does not supersedes the good old syntax and
derivation above it combination.
For one, it allows us to manipulate syntax in an untyped way, and later show
that this manipulation is type-preserving. This is at the core of program
translations as we will see in \nrefch{translations} and become crucial when
derivations are \emph{big}\sidenote{This is the case in the translation I show
in \arefpart{elim-reflection}} since terms (as in untyped syntax) are usually
much smaller. Another point is that it doesn't really make sense to write a
type-checker for a well-typed syntax, something that I expose in
\arefpart{coq-in-coq}.