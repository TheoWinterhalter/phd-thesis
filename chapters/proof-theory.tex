% \setchapterpreamble[u]{\margintoc}
\chapter{Proof theory}
\labch{proof-theory}

My work is done in the wide domain of proof theory. Proof theorists are
interested in the way to prove things, in the `proof' object itself.
This allows us to understand more about proof reuse, about transposition of a
property to another system, or about some intrinsinc properties of the proof
itself. The study of proofs also allows to define clear systems outlining
formally what a proof is and when it is valid. This leads to the notion of
certificate that one can check independently. The epitome of this is the ability
to write proofs that are checkable by a computer, shifting the trust one needs
to put within every proof, to the system which validates them all.

\section{How to prove something}

\subsection{A social construct?}

Before we start proving something, we must know precisely what it is we want to
prove. In informal mathematics, the statement will be a sentence, involving
concepts that the writer and reader agree on. The proof then consists in a
sequence of sentences and argument that convince the same readers.

With this definition, a proof is a subjective concept, it depends on the
reader's capacity to understand and potentially fill the gap themselves about
understood statements and properties. It also usually involves a fair bit of
\emph{trusting}: you may not understand a proof, but will believe in
the common effort of the community to verify the proof or, even better,
reproduce it.
As such, \emph{consensus} seems key in the scientific community.

One way to reach consensus much faster is to have statements and proofs really
precise and unambiguous, described in formal systems. This approach still has
shortcomings, will all readers check every tiny detail of the proof once it's
laid out extensively? This poses the risk of having the \emph{idea} lost in a
sea of information.
To me this calls for computer-verified proofs---and maybe even automated or
computer aided proofs---so that the reader can focus on the interesting part of
the proof while trusting only the tool and not the human that used it.

Even then, there is room for question on whether this really constitutes a
proof.
For instance, how \emph{hard} is it for the computer to \emph{see} that the
proof is indeed correct? One definition would be to say, as long as it takes
a finite amount of time, it's good, but if it takes ages, we won't have any
certainty. In \sidecite{de1991plea}, de Bruijn suggests that a proof should be
self-evident. You shouldn't have to think for hours before seeing that is indeed
correct, and the same holds for computers.

In the remainder of the section I will address the way we \emph{write}
statements and proofs.

\subsection{Formal statements}

\marginnote[0.2cm]{
  Since I want to be as general as possible, this talk about formal statements
  will be pretty informal.
}
What do formal statements look like? Probably something like
\[
  \forall n \in \mathbb{N}. \exists m \in \mathbb{R}.
  f(n) = \mathsf{e}^{\phi(m)} \wedge g(n) > m
\]
It involves defined symbols, and logical connectives to make something precise.
In particular you will note the universal (\(\forall\)) and existential
(\(\exists\)) quantifiers, equality (\(=\)), logical conjunction (\(\wedge\))
and a comparison operator (\(>\)).
We can assume \(\mathbb{N}\), \(\mathbb{R}\), \(f\), \(g\), \(h\), \(\phi\) and
\(e\) to be defined prior to the statement (using similar formalism).

To define this, we give a syntax of propositions, mutually with a syntax of
sets on which we want to quantify. Because equality can mention elements however
we also have to provide a syntax for those, and maybe one for function symbols.
\marginnote[0.7cm]{
  Here \(P \to Q\) denotes `\(P\) implies \(Q\)', often written
  \(P \Longrightarrow Q\). In my domain things are different and we use a single
  arrow.
}
\[
  \begin{array}{rrl}
    P, Q &\bnf& \top \bnfor \bot \bnfor P \wedge Q \bnfor P \vee Q
    \bnfor P \to Q \\
    &\bnfor& \forall x \in E. P \bnfor \exists x \in E. P \bnfor u = v \\
    E &\bnf& \mathbb{N} \bnfor \mathbb{R} \bnfor \dots \\
    u, v &\bnf& x \bnfor \mathsf{e}^u \bnfor u + v \bnfor f(u) \bnfor \dots \\
    f, g, h &\bnf& \dots
  \end{array}
\]

Coming up with a correct syntax like those can be pretty painful so formalisms
tend to be as minimal as possible, the other advantage being that it is much
easier to reason on the statements when there aren't hundreds of syntactical
constructs.

Of course, giving a syntax of statements is not enough. We must give these
symbols a semantics to know what it means to prove them.

\subsection{Inference rules}

We need to define what it means to prove a statement given some hypotheses.
For instance we have
\[
  A, B \vdash A \wedge B
\]
to denote the fact that \(A\) and \(B\) as hypotheses, \emph{entail} the
proposition \(A \wedge B\).
\marginnote[0.2cm]{
  The setting with only one proposition on the right is called
  \emph{natural deduction} and was introduced by Gentzen; with a list of
  propositions it is instead called the \emph{sequent calculus}.
}
This is called a \emph{judgement}, and in general it can have a list of
statements on the right: \(A, B, C \vdash D, E, F\) means that, \emph{assuming}
\(A\), \(B\) \emph{and} \(C\), then \emph{either} \(D\), \(E\) \emph{or} \(F\)
is provable.
I will focus mostly on cases where the right-hand side consists only
of one proposition.
\[
  \begin{array}{rcl}
    \Ga, \D &\bnf& \ctxempty \bnfor \Ga, P \\
    \mathcal{J} &\bnf& \Ga \vdash \D
  \end{array}
\]
We can now move on to the notion of inference rules. They are what defines the
logic in which we place ourselves, they dictate what judgements can be
\emph{derived}---\ie proven---and how.
The simplest of rules usually is the so-called \emph{axiom} rule stating that
assuming \(A\), you can prove \(A\).
\marginnote[-0.3cm]{
  Alternatively we could consider
  \[
    \infer
      {A \in \Ga}
      {\Ga \vdash A}
  \]
}
\[
  \infer
    { }
    {A \vdash A}
\]
The line separates one judgement below, the conclusion, to one, several or
possibly no judgements above. They represent requirements to conclude the below
part. In this case, one doesn't need to assume anything to conclude that \(A\)
entails \(A\).
Going back to conjunction, the rule\sidenote{It is actually one of many possible
rules. It all depends on the logic.} to prove one is the following.
\[
  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash B
    }
    {\Ga \vdash A \wedge B}
\]
That is to say, to prove \(A \wedge B\) (under hypotheses \(\Ga\)), it
\emph{suffices} to prove \(A\) and \(B\) (under the same hypotheses).
This is called an \emph{introduction} rule because it allows us to introduce a
connective in the conclusion.
Sometimes, we want to be able to conclude something from a complex assumption
like \(A \wedge B\), this is instead called an \emph{elimination} rule.
\begin{mathpar}
  \infer
    {\Ga, A, B \vdash P}
    {\Ga, A \wedge B \vdash P}
  %
\end{mathpar}
To prove \(P\) assuming \(A \wedge B\), it is enough to prove \(P\) assuming
both \(A\) and \(B\).
We then have similar rules for disjunction.
\marginnote[0.5cm]{
  To prove \(A \vee B\) you only need to prove either \(A\) or \(B\), hence the
  two introduction rules. On the contrary if you want to prove \(P\) assuming
  \(A \vee B\), you have to provide a proof for the two different cases: either
  \(A\) holds, or \(B\) holds, in both instances \(P\) should hold.
}
\begin{mathpar}
  \infer
    {\Ga \vdash A}
    {\Ga \vdash A \vee B}
  %

  \infer
    {\Ga \vdash B}
    {\Ga \vdash A \vee B}
  %

  \infer
    {
      \Ga, A \vdash P \\
      \Ga, B \vdash P
    }
    {\Ga, A \vee B \vdash P}
  %
\end{mathpar}

Amongst the most important rules are those related to implication.
\begin{mathpar}
  \infer
    {\Ga, A \vdash B}
    {\Ga \vdash A \to B}
  %

  \infer
    {
      \Ga \vdash A \to B \\
      \Ga \vdash A
    }
    {\Ga \vdash B}
  %
\end{mathpar}
They are interesting because they show a level of interaction between the
entailement and the implication. To prove that \(A \to B\), you only assume
\(A\) and show \(B\). Moreover, if you know \(A \to B\) and \(A\), then you
have \(B\). This last rule is called the \emph{modus ponens}.

Because of this, it might seem like implication (\(\to\)), entailement
(\(\vdash\)) and inference (---) are three forms of logical implication.
They \emph{are}, but at different levels.

\section{Proof frameworks}

There are many proof frameworks and logics and I will not review all of them,
but I will present the most relevant to the rest of my work.

\subsection{Classical logic}

Classical logic is the logic most people have in mind where any proposition is
either ``true'' or ``false'' (but not either provable or provably
contradictory). Gentzen describe it in a sequent calculus called
\LK~\sidecite{gentzen1935untersuchungen}.

The syntax is given by
\[
  \begin{array}{rrl}
    P, Q, A, B &\bnf& \top \bnfor \bot \bnfor P \wedge Q \bnfor P \vee Q
    \bnfor P \to Q \bnfor \neg P \\
    \Ga, \D &\bnf& \ctxempty \bnfor \Ga, P \\
    \mathcal{J} &\bnf& \Ga \vdash \D
  \end{array}
\]
which corresponds to \emph{propositional logic}.
I will give the rules in separate bundles: the first of which consists of
the two important rules of \emph{axiom} and \emph{cut}.
\marginnote[1cm]{
  Next to the rules I write their name in parentheses.
  \(I\) stands for identity.
}
\begin{mathpar}
  \infer
    { }
    {A \vdash A}
  (I)

  \infer
    {
      \Ga \vdash \D, A \\
      A, \Xi \vdash \Theta
    }
    {\Ga, \Xi \vdash \D, \Theta}
  (Cut)
\end{mathpar}
While the \emph{axiom} rule has nothing special about it, the \emph{cut} rule
might seem a bit peculiar. If we forget about \(\D\) and \(\Xi\) it says that if
\(\Ga\) entails \(A\) which itself entails \(\Theta\) then \(\Ga\) entails
\(\Theta\), which is not so far from the \emph{modus ponens} after all.

Then we have the logical rules---\ie that pertain to the logical constructors
of propositional logic:
\marginnote[1cm]{
  The \(L\)s and \(R\)s in rules indicate whether it operates on the left or
  right of the sequent.
}
\begin{mathpar}
  \infer
    {\Ga, A \vdash \D}
    {\Ga, A \wedge B \vdash \D}
  (\wedge L_1)

  \infer
    {\Ga \vdash A, \D}
    {\Ga \vdash A \vee B, \D}
  (\vee R_1)

  \infer
    {\Ga,B \vdash \D}
    {\Ga, A \wedge B \vdash \D}
  (\wedge L_2)

  \infer
    {\Ga \vdash B, \D}
    {\Ga \vdash A \vee B, \D}
  (\vee R_2)

  \infer
    {
      \Ga, A \vdash \D \\
      \Xi, B \vdash \Theta
    }
    {\Ga, \Xi, A \vee B \vdash \D, \Theta}
  (\vee L)

  \infer
    {
      \Ga \vdash A,\D \\
      \Xi \vdash B, \Theta
    }
    {\Ga, \Xi \vdash A \wedge B, \D, \Theta}
  (\wedge R)

  \infer
    {
      \Ga \vdash A, \D \\
      \Xi, B \vdash \Theta
    }
    {\Ga, \Xi, A \to B \vdash \D, \Theta}
  (\to L)

  \infer
    {\Ga, A \vdash B, \D}
    {\Ga \vdash A \to B, \D}
  (\to R)

  \infer
    {\Ga \vdash A, \D}
    {\Ga, \neg A \vdash \D}
  (\neg L)

  \infer
    {\Ga, A \vdash \D}
    {\Ga \vdash \neg A, \D}
  (\neg R)

  \infer
    {\Ga \vdash \D}
    {\Ga, \top \vdash \D}
  (\top L)

  \infer
    { }
    {\Ga \vdash \top, \D}
  (\top R)

  \infer
    { }
    {\Ga, \bot \vdash \D}
  (\bot L)

  \infer
    {\Ga \vdash \D}
    {\Ga \vdash \bot, \D}
  (\bot R)
\end{mathpar}

The rest are structural rules that allow us to weaken the hypotheses or the
conclusion, or reorder propositions and remove duplicates.
\marginnote[1cm]{
  \(W\) stands for weakening, \(C\) for contraction and \(P\) for permutation.
}
\begin{mathpar}
  \infer
    {\Ga \vdash \D}
    {\Ga, A \vdash \D}
  (WL)

  \infer
    {\Ga \vdash \D}
    {\Ga \vdash A, \D}
  (WR)

  \infer
    {\Ga, A, A \vdash \D}
    {\Ga, A \vdash \D}
  (CL)

  \infer
    {\Ga \vdash A, A, \D}
    {\Ga \vdash A, \D}
  (CR)

  \infer
    {\Ga, A, B, \Xi \vdash \D}
    {\Ga, B, A, \Xi \vdash \D}
  (PL)

  \infer
    {\Ga \vdash \D, A, B, \Theta}
    {\Ga \vdash \D, B, A, \Theta}
  (PR)
\end{mathpar}

\LK is classical because we can derive the \acrlong{LEM} from it.
\begin{mathpar}
  \infer*[Right=\((CR)\)]
    {
      \infer*[Right=\((\vee R_1)\)]
        {
          \infer*[Right=\((PR)\)]
            {
              \infer*[Right=\((\vee R_2)\)]
                {
                  \infer*[Right=\((\neg R)\)]
                    {
                      \infer*[Right=\((I)\)]
                        { }
                        {P \vdash P}
                    }
                    {\vdash \neg P, P}
                }
                {\vdash P \vee \neg P, P}
            }
            {\vdash P, P \vee \neg P}
        }
        {\vdash P \vee \neg P, P \vee \neg P}
    }
    {\vdash P \vee \neg P}
\end{mathpar}
\LK also features double negation elimination:
\[
  \infer*[Right=\((\to R)\)]
    {
      \infer*[Right=\((\neg L)\)]
        {
          \infer*[Right=\((\neg R)\)]
            {
              \infer*[Right=\((I)\)]{ }{P \vdash P}
            }
            {\vdash \neg P, P}
        }
        {\neg \neg P \vdash P}
    }
    {\vdash \neg \neg P \to P}
\]

\subsection{Intuitionistic logic}

Intuitionistic logic doesn't abide by the \acrshort{LEM}, so its treatment of
negation is a bit different so that double negation cannot be eliminated as
above. As such we define \LJ in natural deduction (the sequent style was also
of importance to get classical logic), on the same syntax of propositional
logic.
It is pretty similar.
\begin{mathpar}
  \infer
    { }
    {A \vdash A}
  (I)

  \infer
    {
      \Ga \vdash A \\
      A, \D \vdash B
    }
    {\Ga, \D \vdash B}
  (Cut)
\end{mathpar}

We have less rules because some don't make sense anymore with only one
proposition on the right.
\begin{mathpar}
  \infer
    {\Ga, A, B \vdash C}
    {\Ga, A \wedge B \vdash C}
  (\wedge L)

  \infer
    {\Ga \vdash A}
    {\Ga \vdash A \vee B}
  (\vee R_1)

  \infer
    {\Ga \vdash B, \D}
    {\Ga \vdash A \vee B, \D}
  (\vee R_2)

  \infer
    {
      \Ga, A \vdash C \\
      \Ga, B \vdash C
    }
    {\Ga, A \vee B \vdash C}
  (\vee L)

  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash B
    }
    {\Ga \vdash A \wedge B}
  (\wedge R)

  \infer
    {
      \Ga \vdash A \\
      \D, B \vdash C
    }
    {\Ga, \D, A \to B \vdash C}
  (\to L)

  \infer
    {\Ga, A \vdash B}
    {\Ga \vdash A \to B}
  (\to R)

  \infer
    {\Ga \vdash A}
    {\Ga, \neg A \vdash B}
  (\neg L)

  \infer
    {\Ga, A \vdash \bot}
    {\Ga \vdash \neg A}
  (\neg R)

  \infer
    {\Ga \vdash A}
    {\Ga, \top \vdash A}
  (\top L)

  \infer
    { }
    {\vdash \top}
  (\top R)

  \infer
    { }
    {\Ga, \bot \vdash A}
  (\bot L)
\end{mathpar}

The structural rules are only happening on the left now since there is nothing
to rearrange on the right.
\begin{mathpar}
  \infer
    {\Ga \vdash B}
    {\Ga, A \vdash B}
  (W)

  \infer
    {\Ga, A, A \vdash B}
    {\Ga, A \vdash B}
  (C)

  \infer
    {\Ga, A, B, \D \vdash A}
    {\Ga, B, A, \D \vdash A}
  (P)
\end{mathpar}

This logic is constructive in that from every proof derivation one can extract
a proof where the last rule is an introduction rule, \ie a \emph{right} rule.
This is obtained by a process known as \emph{cut-elimination} which removes all
occurrences of the \((Cut)\) rule in a derivation, showing at the same time that
it is \emph{admissible} or redundant.

\subsection{Mechanised proofs}

Once we have a formal logic it makes sense to \emph{teach} its rules to a
computer to use it for what we call \emph{mechanised proofs}.
We nowadays have many of those, be it under the name \emph{proof assistants} or
\emph{automated theorem provers}.

\paragraph{Automated theorem provers} are tools that will take statements as
inputs and attempt to prove or disprove them, sometimes taking hints from the
user when stuck.
They are very attractive because the user usually doesn't have to learn about
the logic involved, just trust that it \emph{works}\sidenote{Ideally that it is
consistent}. However it can be a hassle when they fail to prove or disprove
something and the user has to figure out the right way to state things so that
the tool manages to progress.

\paragraph{Proof assistants} require more work from the part of the user but are
usually more malleable and robust. They constitute a framework in which the user
can state and prove lemmata. Again there are several proof assistants: \IsaHOL
is one of the most commonly used and based on
\acrfull{HOL}~\sidecite[-1.35cm]{10.5555/1791547}, but I am more  familiar with
proof assistants based on type theory such as \Coq~\sidecite[-1cm]{coq} and
\Agda~\sidecite[-0.6cm]{norell2007towards}.
The next introductory chapters will focus on this notion of proof framework.

\section{Limitations: Gödel's incompleteness theorems}

When you have a proof framework or logic, there are two main things you want to
know about it:
\begin{itemize}
  \item Is it \emph{consistent}? That is are there things my system doesn't
  prove?
  \item Is it \emph{complete}? That is, are all propositions either provable
  or provably contradictory (\ie the negation is provable)?
\end{itemize}

Consistency can be reformulated equivalently as the absence of proof of false
\(\bot\).
Completeness is not to be confused with the \acrshort{LEM} which gives a proof
of \(P \vee \neg P\) for each \(P\) but not a proof of either \(P\) or
\(\neg P\) for every \(P\).

Ideally one would want their favourite system to enjoy \emph{both} of these
properties, the proof of which would be conducted in the very same system.
In 1931~\sidecite{godel1931formal} Gödel shattered all hopes of ever
accomplishing that.

\begin{theorem}[Gödel's first incompleteness theorem]
  Every consistent formal system that supports arithmetic is incomplete.
\end{theorem}

\marginnote[1cm]{
  Some people still work in inconsistent logics so let's stress the
  \emph{usually}.
}
This means, that, except in very small systems, consistency and completeness
cannot hold at the same time. Usually we go with consistency because
inconsistent systems are trivially complete (every property is provable) and not
of much interest.

\begin{theorem}[Gödel's second incompleteness theorem]
  Every consistent formal system that supports arithmetic cannot prove its
  own consistency.
\end{theorem}
A system that is able to prove itself consistent is actually inconsistent.
This means that we have to rely on the \emph{social construct} I mentioned
ealier, because the system in which you prove one system consistent is stronger
and needs to be trusted as well.

In both these theorems we talk about the fact that the theory should support
arithmetic for them to apply. By arithmetic we mean Robinson
arithmetic~\sidecite{robinson1950essentially} rather than Peano arithmetic,
as proved in~\sidecite[0.7cm]{bezboruah1976godel}.
It is an axiomatisation of a set \(\mathbb{N}\) with distinguished member
\(0\) and a successor (unary) operation on \(\mathbb{N}\) written \(\So\).
Moreover it features two binary operations called addition and multiplication
written with infix notations \(+\) and \(\times\).
The axioms are the following.
\[
  \begin{array}{ll}
    \forall x.& \So\ x \not= 0 \\
    \forall x\ y.& \So\ x = \So\ y \to x = y \\
    \forall x.& x = 0 \vee \exists y.\ x = \So\ y \\
    \forall x.& x + 0 = x \\
    \forall x\ y.& x + \So\ y = \So\ (x + y) \\
    \forall x.& x \times 0 = 0 \\
    \forall x\ y.& x \times \So\ y = x \times y + x
  \end{array}
\]

I have not made the statements of Gödel's incompleteness theorems very precise
as it is beyond the scope of my work. It is very easy to find references that
are not in German on the subject.

Whether---or rather \emph{how}---the original argument adapts to the setting of
type theory is still an open problem, but it is so mainly for technical reasons
and it is widely believed that the same---or very similar restrictions---apply
to it.