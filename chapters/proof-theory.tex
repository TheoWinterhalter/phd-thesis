% \setchapterpreamble[u]{\margintoc}
\chapter{Proof theory}
\labch{proof-theory}

My work is done in the wide domain of proof theory. Proof theorists are
interested in the way to prove things, in the `proof' object itself, \eg we
might want to check that a proof uses some specific rules of deduction.
We may also want to prove that a system is not contradictory, \ie that it does
not entail both an assertion and its negation.
This allows us to understand more about proof reuse, about transposition of a
property to another system, or about some intrinsinc properties of the proof
itself. The study of proofs also allows to define clear systems outlining
formally what a proof is and when it is valid. This leads to the notion of
certificate that one can check independently. The epitome of this is the ability
to write proofs that are checkable by a computer, shifting the trust one needs
to put within every proof, to the system which validates them all.

\section{How to prove something}

\subsection{A social construct?}

Before we start proving something, we must know precisely what it is we want to
prove. In informal mathematics, the statement will be a sentence, involving
concepts that the writer and reader agree on. The proof then consists in a
sequence of sentences and argument that convince the same readers.

For instance, the Pythagorean theorem states that
``the area of the square whose side is the hypothenuse of a right triangle is
equal to the sum of the areas of the squares on the other two sides.''

With this definition, a proof is a subjective concept, it depends on the
reader's capacity to understand and potentially fill the gap themselves about
understood statements and properties. It also usually involves a fair bit of
\emph{trusting}: you may not understand a proof, but will believe in
the common effort of the community to verify the proof or, even better,
reproduce it.
As such, \emph{consensus} seems key in the scientific community.

One way to reach consensus much faster is to have statements and proofs really
precise and unambiguous, described in formal systems. This approach still has
shortcomings, will all readers check every tiny detail of the proof once it's
laid out extensively? This runs the risk of having the \emph{idea} lost in a
sea of information.
To me, this calls for computer-verified proofs---and maybe even automated or
computer aided proofs---coming with paper proofs exposing the ideas
so that the reader can focus on the interesting part of
the proof while trusting only the tool and not the human that used it.

Even then, there is room for question on whether this really constitutes a
proof.
For instance, how \emph{hard} is it for the computer to \emph{see} that the
proof is indeed correct? One definition would be to say, as long as it takes
a finite amount of time, it is good, but if it takes ages, we will not have any
certainty. In \sidecite{de1991plea}, de Bruijn suggests that a proof should be
self-evident. You should not have to think for hours before seeing that is
indeed correct, and the same holds for computers.

In the remainder of the section I will address the way we \emph{write}
statements and proofs.

\subsection{Formal statements}

\marginnote[0.2cm]{
  Since I want to be as general as possible, this talk about formal statements
  will be pretty informal.
}
What do formal statements look like? Probably something like
\[
  \forall n \in \mathbb{N}. \exists m \in \mathbb{R}.
  f(n) = \mathsf{e}^{\phi(m)} \wedge g(n) > m
\]
It involves defined symbols, and logical connectives to make something precise.
In particular you will note the universal (\(\forall\)) and existential
(\(\exists\)) quantifiers, equality (\(=\)), logical conjunction (\(\wedge\))
and a comparison operator (\(>\)).
We can assume \(\mathbb{N}\), \(\mathbb{R}\), \(f\), \(g\), \(h\), \(\phi\) and
\(e\) to be defined prior to the statement (using similar formalism).

To define this, we give a syntax of propositions, mutually with a syntax of
sets on which we want to quantify. Because equality can mention elements however
we also have to provide a syntax for those, and maybe one for function symbols.
\marginnote[0.7cm]{
  Here \(P \to Q\) denotes `\(P\) implies \(Q\)', often written
  \(P \Longrightarrow Q\). In my domain things are different and we use a single
  arrow.
}
\[
  \begin{array}{rrl}
    P, Q &\bnf& \top \bnfor \bot \bnfor P \wedge Q \bnfor P \vee Q
    \bnfor P \to Q \\
    &\bnfor& \forall x \in E. P \bnfor \exists x \in E. P \bnfor u = v \\
    E &\bnf& \mathbb{N} \bnfor \mathbb{R} \bnfor \dots \\
    u, v &\bnf& x \bnfor \mathsf{e}^u \bnfor u + v \bnfor f(u) \bnfor \dots \\
    f, g, h &\bnf& \dots
  \end{array}
\]

\marginnote[0.5cm]{
  Parentheses are not part of the syntax but are there to lift any ambiguity
  and distinguish \(P \wedge (Q \vee R)\) from \((P \wedge Q) \vee R\)
  for example.
}
This presentation of syntax is called \acrfull{BNF} and it says that there are
expressions that we will write with \(P\) or \(Q\) that can be either \(\top\)
or \(\bot\) or \(P \wedge Q\) where \(P\) and \(Q\) are both of the same form,
and so on.
For instance, \(\bot \wedge (\top \vee (\top \to \bot))\) is one such
expression.

Here \(P,Q\) are propositions, \(E\) stands for a set, \(u,v\) are mathematical
expressions whereas \(f,g,h\) are function symbols.

Logical connectives (\(\wedge, \vee, \forall\), \dots) and operations
(\(+, \times\), \dots) are the building blocks of statements, as such we call
them \emph{constructors}.

Coming up with a correct syntax like those can be pretty painful so formalisms
tend to be as minimal as possible, the other advantage being that it is much
easier to reason on the statements when there are not hundreds of syntactical
constructs.

Of course, giving a syntax of statements is not enough. We must give these
symbols a semantics (\ie a meaning) to know what it means to prove them.
For instance, we might want to sepcify that the symbol \(+\) has the properties
which are expected of addition.

\subsection{Inference rules}

We need to define what it means to prove a statement given some hypotheses.
For instance we have
\[
  A, B \vdash A \wedge B
\]
to denote the fact that \(A\) and \(B\) as hypotheses, \emph{entail} the
proposition \(A \wedge B\) (read `\(A\) \emph{and} \(B\)').
\marginnote[0.2cm]{
  There are also settings where several propositions appear on the right-hand
  side: \(A, B \vdash C, D\) means that assuming \(A\) and \(B\) then either
  \(C\) or \(D\) hold.
}
This is called a \emph{judgement}: \(A, B, C \vdash D\) means that,
\emph{assuming} \(A\), \(B\) \emph{and} \(C\), then \(D\) holds.
\[
  \begin{array}{rcl}
    \Ga, \D &\bnf& \ctxempty \bnfor \Ga, P \\
    \mathcal{J} &\bnf& \Ga \vdash P
  \end{array}
\]
We can now move on to the notion of \emph{inference} rule. They are what defines
the logic that we consider, they dictate what judgements can be
\emph{derived}---\ie proven---and how.
The simplest of rules usually is the so-called \emph{axiom} rule stating that
assuming \(A\), you can prove \(A\).
% \marginnote[-0.3cm]{
%   Alternatively we could consider
%   \[
%     \infer
%       {A \in \Ga}
%       {\Ga \vdash A}
%   \]
% }
\[
  \infer
    { }
    {A \vdash A}
\]
or more generally
\[
  \infer
    {A \in \Ga}
    {\Ga \vdash A}
\]
The line separates one judgement below, the conclusion, to one, several or
possibly no judgements above. They represent requirements to conclude the below
part. In this case, one does not need to assume anything to conclude that \(A\)
entails \(A\). The condition \(A \in \Ga\) is not a judgement itself but really
a side condition.
Going back to conjunction, the rule\sidenote{It is actually one of many possible
rules. It all depends on the logic.} to prove one is the following.
\[
  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash B
    }
    {\Ga \vdash A \wedge B}
\]
That is to say, to prove \(A \wedge B\) (under hypotheses \(\Ga\)), it
\emph{suffices} to prove \(A\) and \(B\) (under the same hypotheses).
This is called an \emph{introduction} rule because it allows us to introduce a
connective in the conclusion.
\marginnote[0.3cm]{
  Once again, this is one of many ways to proceed.
  This setting corresponds to \emph{natural deduction} and was introduced by
  Gentzen; he also introduced \emph{sequent calculus} where there are no
  elimination rules, but introduction rules on the left and on the right.
}
Sometimes, we want to be able to conclude something from a complex assumption
like \(A \wedge B\), this is instead called an \emph{elimination} rule.
\begin{mathpar}
  \infer
    {\Ga \vdash A \wedge B}
    {\Ga \vdash A}
  %

  \infer
    {\Ga \vdash A \wedge B}
    {\Ga \vdash B}
  %
\end{mathpar}
If you can prove \(A \wedge B\) then you can prove \(A\) (you can also prove
\(B\) but that is another rule).
We then have similar rules for disjunction
(\(A \vee B\) reads `\(A\) \emph{or} \(B\)').
\marginnote[0.5cm]{
  To prove \(A \vee B\) you only need to prove either \(A\) or \(B\), hence the
  two introduction rules. On the contrary if you want to prove \(P\) knowing
  \(A \vee B\), you have to provide a proof for the two different cases: either
  \(A\) holds, or \(B\) holds, in both instances \(P\) should hold.
}
\begin{mathpar}
  \infer
    {\Ga \vdash A}
    {\Ga \vdash A \vee B}
  %

  \infer
    {\Ga \vdash B}
    {\Ga \vdash A \vee B}
  %

  \infer
    {
      \Ga \vdash A \vee B \\
      \Ga, A \vdash P \\
      \Ga, B \vdash P
    }
    {\Ga \vdash P}
  %
\end{mathpar}

Amongst the most important rules are those related to implication.
\begin{mathpar}
  \infer
    {\Ga, A \vdash B}
    {\Ga \vdash A \to B}
  %

  \infer
    {
      \Ga \vdash A \to B \\
      \Ga \vdash A
    }
    {\Ga \vdash B}
  %
\end{mathpar}
They are interesting because they show a level of interaction between the
entailement and the implication. To prove that \(A \to B\), you only assume
\(A\) and show \(B\). Moreover, if you know \(A \to B\) and \(A\), then you
have \(B\). This last rule is called the \emph{modus ponens}, that is the
elimination of the implication.

% Because of this, it might seem like implication (\(\to\)), entailement
% (\(\vdash\)) and inference (---) are three forms of logical implication.
% They \emph{are}, but at different levels.

\section{Proof frameworks}

There are many proof frameworks and logics and I will not review all of them,
but I will present the most relevant ones to the rest of my work.

\subsection{Classical logic}

Classical logic is the logic most people have in mind where any proposition is
either ``true'' or ``false'' (but not either provable or provably
contradictory).
% Gentzen describe it in a sequent calculus called
% \LK~\sidecite{gentzen1935untersuchungen}.
Gentzen describes a natural deduction system called
\NK~\sidecite{gentzen1935untersuchungen}.

The syntax is given by
\[
  \begin{array}{rrl}
    P, Q, A, B &\bnf& \top \bnfor \bot \bnfor P \wedge Q \bnfor P \vee Q
    \bnfor P \to Q \bnfor \neg P \\
    \Ga, \D &\bnf& \ctxempty \bnfor \Ga, P \\
    \mathcal{J} &\bnf& \Ga \vdash \D
  \end{array}
\]
which corresponds to \emph{propositional logic}.
Notice how the right-hand side of judgements contains a context, \ie a list of
propositions, rather than just one as we previously saw.

I will give the rules in two separate bundle.
We first have the logical rules---\ie that pertain to the logical constructors
of propositional logic:
\marginnote[1cm]{
  Next to the rules I write their name in parentheses.
  \(ax\) stands for `axiom'.
  The \(I\)s and \(E\)s in rules indicate whether each rule is an introduction
  rule or an elimination rule.
}
\begin{mathpar}
  \infer
    {A \in \Ga}
    {\Ga \vdash A}
  (ax)

  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash B
    }
    {\Ga \vdash A \wedge B}
  (\wedge I)

  \infer
    {\Ga \vdash A \wedge B}
    {\Ga \vdash A}
  (\wedge E_1)

  \infer
    {\Ga \vdash A \wedge B}
    {\Ga \vdash B}
  (\wedge E_2)

  \infer
    {\Ga \vdash A, B, \D}
    {\Ga \vdash A \vee B, \D}
  (\vee I)

  \infer
    {\Ga \vdash A \vee B, \D}
    {\Ga \vdash A, B, \D}
  (\vee E)

  \infer
    {\Ga, A \vdash B, \D}
    {\Ga \vdash A \to B, \D}
  (\to I)

  \infer
    {
      \Ga \vdash A \to B, \D \\
      \Xi \vdash A, \Theta
    }
    {\Ga, \Xi \vdash \D, \Theta}
  (\to E)

  \infer
    {\Ga, A \vdash \D}
    {\Ga \vdash \neg A, \D}
  (\neg I)

  \infer
    {
      \Ga \vdash \neg A, \D \\
      \Xi \vdash A, \Theta
    }
    {\Ga, \Xi \vdash \D, \Theta}
  (\neg E)

  \infer
    { }
    {\vdash \top}
  (\top I)

  \infer
    {\Ga \vdash \D}
    {\Ga \vdash \bot, \D}
  (\bot I)

  \infer
    {\Ga \vdash \bot, \D}
    {\Ga \vdash \D}
  (\bot E)
\end{mathpar}

The rest are structural rules that allow us to weaken the hypotheses (\ie add
a new, unused hypothesis in the context) or the conclusion, or reorder
propositions and remove duplicates.
\marginnote[1cm]{
  \(W\) stands for weakening, \(C\) for contraction and \(P\) for permutation
  while \(L\) and \(R\) stand for left and right.
}
\begin{mathpar}
  \infer
    {\Ga \vdash \D}
    {\Ga, A \vdash \D}
  (WL)

  \infer
    {\Ga \vdash \D}
    {\Ga \vdash A, \D}
  (WR)

  \infer
    {\Ga, A, A \vdash \D}
    {\Ga, A \vdash \D}
  (CL)

  \infer
    {\Ga \vdash A, A, \D}
    {\Ga \vdash A, \D}
  (CR)

  \infer
    {\Ga, A, B, \Xi \vdash \D}
    {\Ga, B, A, \Xi \vdash \D}
  (PL)

  \infer
    {\Ga \vdash \D, A, B, \Theta}
    {\Ga \vdash \D, B, A, \Theta}
  (PR)
\end{mathpar}

\NK is classical because we can derive the \acrfull{LEM} from it.
\begin{mathpar}
  \infer*[Right=\(\vee I\)]
    {
      \infer*[Right=\(PR\)]
        {
          \infer*[Right=\(\neg I\)]
            {
              \infer*[Right=\(ax\)]
                { }
                {P \vdash P}
            }
            {\vdash \neg P, P}
        }
        {\vdash P, \neg P}
    }
    {\vdash P \vee \neg P}
\end{mathpar}

\NK also features double negation elimination:
\[
  \infer*[Right=\((\to I)\)]
    {
      \infer*[Right=\((\neg E)\)]
        {
          \infer*[Right=\((ax)\)]
            { }
            {\neg \neg P \vdash \neg \neg P}
          \\
          \infer*[Right=\((\neg I)\)]
            {
              \infer*[Right=\((ax)\)]
                { }
                {P \vdash P}
            }
            {\vdash \neg P, P}
        }
        {\neg \neg P \vdash P}
    }
    {\vdash \neg \neg P \to P}
\]

\subsection{Intuitionistic logic}

Intuitionistic logic does not abide by the \acrlong{LEM}, so its treatment of
negation is a bit different so that double negation cannot be eliminated as
above. As such we define \NJ in natural deduction, on the same syntax of
propositional logic. The main difference is that we revert to only one
propositions on the right-hand side of judgements.
It is pretty similar to \NK.
\begin{mathpar}
  \infer
    {A \in \Ga}
    {\Ga \vdash A}
  (ax)

  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash B
    }
    {\Ga \vdash A \wedge B}
  (\wedge I)

  \infer
    {\Ga \vdash A \wedge B}
    {\Ga \vdash A}
  (\wedge E_1)

  \infer
    {\Ga \vdash A \wedge B}
    {\Ga \vdash B}
  (\wedge E_2)

  \infer
    {\Ga \vdash A}
    {\Ga \vdash A \vee B}
  (\vee I_1)

  \infer
    {\Ga \vdash B}
    {\Ga \vdash A \vee B}
  (\vee I_2)

  \infer
    {
      \Ga \vdash A \vee B \\
      \Ga, A \vdash C \\
      \Ga, B \vdash C
    }
    {\Ga \vdash C}
  (\vee E)

  \infer
    {\Ga, A \vdash B}
    {\Ga \vdash A \to B}
  (\to I)

  \infer
    {
      \Ga \vdash A \to B \\
      \Ga \vdash A
    }
    {\Ga \vdash B}
  (\to E)

  \infer
    {\Ga, A \vdash \bot}
    {\Ga \vdash \neg A}
  (\neg I)

  \infer
    {
      \Ga \vdash \neg A \\
      \Ga \vdash A
    }
    {\Ga \vdash \bot}
  (\neg E)

  \infer
    { }
    {\Ga \vdash \top}
  (\top I)

  \infer
    {\Ga \vdash \bot}
    {\Ga \vdash A}
  (\bot E)
\end{mathpar}

The structural rules are only happening on the left now since there is nothing
to rearrange on the right.
\begin{mathpar}
  \infer
    {\Ga \vdash B}
    {\Ga, A \vdash B}
  (W)

  \infer
    {\Ga, A, A \vdash B}
    {\Ga, A \vdash B}
  (C)

  \infer
    {\Ga, A, B, \D \vdash A}
    {\Ga, B, A, \D \vdash A}
  (P)
\end{mathpar}

This logic is constructive in that, from every proof derivation one can extract
a proof where the last rule is an introduction rule.
For instance, every proof of \(A \vee B\) must contain either a proof of \(A\)
or a proof of \(B\).
This is obtained by a process known as \emph{cut-elimination} which removes all
occurrences of the modus ponens \((\to E)\) rule in a derivation, showing at the
same time that it is \emph{admissible} or redundant.

\subsection{Mechanised proofs}

Once we have a formal logic, it makes sense to \emph{teach} its rules to a
computer to use it for what we call \emph{mechanised proofs}.
We nowadays have many of those, be it under the name \emph{proof assistants} or
\emph{automated theorem provers}.

\paragraph{Automated theorem provers} are tools that will take statements as
inputs and attempt to prove or disprove them, sometimes taking hints from the
user when stuck.
They are very attractive because the user usually does not have to learn about
the logic involved, just trust that it \emph{works}\sidenote{Ideally that it is
consistent}. However it can be a hassle when they fail to prove or disprove
something and the user has to figure out the right way to state things so that
the tool manages to progress.

\paragraph{Proof assistants} require more work from the part of the user but are
usually more malleable and robust. They constitute a framework in which the user
can state and prove lemmata. Again there are several proof assistants: \IsaHOL
is one of the most commonly used and based on
\acrfull{HOL}~\sidecite[-1.35cm]{10.5555/1791547}, but I am more  familiar with
proof assistants based on type theory such as \Coq~\sidecite[-1cm]{coq} and
\Agda~\sidecite[-0.4cm]{norell2007towards}.
The next introductory chapters will focus on this notion of proof framework.

\section{Limitations: Gödel's incompleteness theorems}

When you have a proof framework or logic, there are two main things you want to
know about it:
\begin{itemize}
  \item Is it \emph{consistent}? An inconsistent system is one in which we can
  prove that an assertion and its negation.
  \item Is it \emph{complete}? That is, are all propositions either provable
  or provably contradictory (\ie the negation is provable)?
\end{itemize}

Consistency can be reformulated equivalently as the absence of proof of false
\(\bot\).
Completeness is not to be confused with the \acrshort{LEM} which gives a proof
of \(P \vee \neg P\) for each \(P\) but not a proof of either \(P\) or
\(\neg P\) for every \(P\).

Ideally we would our their favourite system to enjoy \emph{both} of these
properties, the proof of which would be conducted in the very same system.
In 1931~\sidecite{godel1931formal} Gödel shattered all hopes of ever
accomplishing that.

\begin{theorem}[Gödel's first incompleteness theorem]
  Every consistent formal system that supports arithmetic is incomplete.
\end{theorem}

\marginnote[1cm]{
  Some people still work in inconsistent logics so let's stress the
  \emph{usually}.
}
This means, that, except in very small systems, consistency and completeness
cannot hold at the same time. Usually we go with consistency because
inconsistent systems are trivially complete (every property is provable) and not
of much interest.

\begin{theorem}[Gödel's second incompleteness theorem]
  Every consistent formal system that supports arithmetic cannot prove its
  own consistency.
\end{theorem}
A system that is able to prove itself consistent is actually inconsistent.
This means that we have to rely on the scientific \emph{consensus} I mentioned
ealier, because the system in which you prove one system consistent is stronger
and needs to be trusted as well.

In both these theorems we talk about the fact that the theory should support
arithmetic for them to apply. By arithmetic we mean Robinson
arithmetic~\sidecite{robinson1950essentially} rather than Peano arithmetic,
as proved in~\sidecite[0.7cm]{bezboruah1976godel}.
It is an axiomatisation of a set \(\mathbb{N}\) with distinguished member
\(0\) and a successor (unary) operation on \(\mathbb{N}\) written \(\So\).
Moreover it features two binary operations called addition and multiplication
written with infix notations \(+\) and \(\times\).
The axioms are the following.
\[
  \begin{array}{ll}
    \forall x.& \So\ x \not= 0 \\
    \forall x\ y.& \So\ x = \So\ y \to x = y \\
    \forall x.& x = 0 \vee \exists y.\ x = \So\ y \\
    \forall x.& x + 0 = x \\
    \forall x\ y.& x + \So\ y = \So\ (x + y) \\
    \forall x.& x \times 0 = 0 \\
    \forall x\ y.& x \times \So\ y = x \times y + x
  \end{array}
\]

The first axiom states that \(0\) cannot be the successor of another natural
number (\ie some \(n + 1\)). The second says that the successor operation is
injective: if the successors of \(x\) and \(y\) are equal, then \(x = y\).
The third says that every natural number is either \(0\) or the sucessor of
another natural number, this means that every natural number is a succession of
\(\So\) and then \(0\).
\[
  \begin{array}{cccccc}
    0 & \So\ 0 & \So\ (\So\ 0) & \So\ (\So\ (\So\ 0))
    & \So\ (\So\ (\So\ (\So\ 0)))
    & \dots \\
    0 & 1 & 2 & 3 & 4 & \dots
  \end{array}
\]

I have not made the statements of Gödel's incompleteness theorems very precise
as it is beyond the scope of my work. It is very easy to find references that
are not in German on the subject, like the well-written Wikipedia page dedicated
to the subject.

% Whether---or rather \emph{how}---the original argument adapts to the setting of
% type theory is still an open problem, but it is so mainly for technical reasons
% and it is widely believed that the same---or very similar restrictions---apply
% to it.