% \setchapterpreamble[u]{\margintoc}
\chapter{Proof theory}
\labch{proof-theory}

My work is done in the wide domain of proof theory. Proof theorists are
interested in the way to prove things, in the `proof' object itself.
This allows us to understand more about proof reuse, about transposition of a
property to another system, or about some intrinsinc properties of the proof
itself. The study of proofs also allows to define clear systems outlining
formally what a proof is and when it is valid. This leads to the notion of
certificate that one can check independently. The epitome of this is the ability
to write proofs that are checkable by a computer, shifting the trust one needs
to put within every proof, to the system which validates them all.

\section{How to prove something}

\subsection{A social construct?}

Before we start proving something, we must know percisely what it is we want to
prove. In informal mathematics, the statement will be a sentence, involving
concepts that the writer and reader agree on. The proof then consists in a
sequence of sentences and argument that convince the same readers.

With this definition, a proof is a subjective concept, it depends on the
reader's capacity to understand and potentially fill the gap themselves about
understood statements and properties. It also involves a fair bit of
\emph{trusting} usually: you may not understand a proof, but will believe in
the common effort of the community to verify the proof or, even better,
reproduce it.
As such, \emph{consensus} seems key in the scientific community.

One way to reach consensus much faster is to have statements and proofs really
precise and unambiguous, described in formal systems. This approach still has
shortcomings, will all readers check every tiny detail of the proof once it's
laid out extensively? This poses the risk of having the \emph{idea} lost in a
sea of information.
To me this calls for computer-verified proofs---and maybe even automated or
computer aided proofs---so that the reader can focus on the interesting part of
the proof while trusting only the tool and not the human that used it.

Even there is room for question on whether this really constitutes a proof.
For instance, how \emph{hard} is it for the computer to \emph{see} that the
proof is indeed correct? One definition would be to say, as long as it takes
a finite amount of time, it's good, but if it takes ages, we won't have any
certainty. In \sidecite{de1991plea}, de Bruijn suggests that a proof should be
self-evident. You shouldn't have to think for hours before seeing that is indeed
correct, and the same holds for computers.

In the remainder of the section I will address the way we \emph{write}
statements and proofs.

\subsection{Formal statements}

\marginnote[0.2cm]{
  Since I want to be as general as possible, this talk about formal statements
  will be pretty informal.
}
What do formal statements look like? Probably something like
\[
  \forall n \in \mathbb{N}. \exists m \in \mathbb{R}.
  f(n) = \mathsf{e}^{\phi(m)} \wedge g(n) > m
\]
It involves defined symbols, and logical connectives to make something precise.
In particular you will note the universal (\(\forall\)) and existential
(\(\exists\)) quantifiers, equality (\(=\)), logical conjunction (\(\wedge\))
and a comparison operator (\(>\)).
We can assume \(\mathbb{N}\), \(\mathbb{R}\), \(f\), \(g\), \(h\), \(\phi\) and
\(e\) to be defined prior to the statement (using similar formalism).

To define this, we give a syntax of propositions, mutually with a syntax of
sets on which we want to quantify. Because equality can mention elements however
we also have to provide a syntax for those, and maybe one for function symbols.
\marginnote[0.7cm]{
  Here \(P \to Q\) denotes `\(P\) implies \(Q\)', often written
  \(P \Longrightarrow Q\). In my domain things are different and we use a single
  arrow.
}
\[
  \begin{array}{rrl}
    P, Q &\bnf& \top \bnfor \bot \bnfor P \wedge Q \bnfor P \vee Q
    \bnfor P \to Q \\
    &\bnfor& \forall x \in E. P \bnfor \exists x \in E. P \bnfor u = v \\
    E &\bnf& \mathbb{N} \bnfor \mathbb{R} \bnfor \dots \\
    u, v &\bnf& x \bnfor \mathsf{e}^u \bnfor u + v \bnfor f(u) \bnfor \dots \\
    f, g, h &\bnf& \dots
  \end{array}
\]

Coming up with a correct syntax like those can be pretty painful so formalisms
tend to be as minimal as possible, the other advantage being that is much easier
to reason on the statements when there aren't hundreds of syntactical
constructs.

Of course, giving a syntax of statements is not enough. We must give these
symbols a semantics to know what it means to prove them.

\subsection{Inference rules}

We need to define what it means to prove a statement given some hypotheses.
For instance we have
\[
  A, B \vdash A \wedge B
\]
to denote the fact that \(A\) and \(B\) as hypotheses, \emph{entail} the
proposition \(A \wedge B\).
\marginnote[0.2cm]{
  The setting with only one proposition on the right is called
  \emph{natural deduction} and was introduced by Gentzen; with a list of
  propositions it is instead called the \emph{sequent calculus}.
}
This is called a \emph{judgement}, and in general it can have a list of
statements on the right: \(A, B, C \vdash D, E, F\) means that, \emph{assuming}
\(A\), \(B\) \emph{and} \(C\), then \emph{either} \(D\), \(E\) \emph{or} \(F\)
is provable.
I will focus mostly on cases where the right-hand side consists only
of one proposition.
\[
  \begin{array}{rcl}
    \Ga, \D &\bnf& \ctxempty \bnfor \Ga, P \\
    \mathcal{J} &\bnf& \Ga \vdash \D
  \end{array}
\]
We can now move on to the notion of inference rules. They are what define the
logic in which we place ourselves, they dictate what judgements can be
\emph{derived}---\ie proven---and how.
The simplest of rules usually is the so-called \emph{axiom} rule stating that
assuming \(A\), you can prove \(A\).
\marginnote[-0.3cm]{
  Alternatively we could consider
  \[
    \infer
      {A \in \Ga}
      {\Ga \vdash A}
  \]
}
\[
  \infer
    { }
    {A \vdash A}
\]
The line separates one judgement below, the conclusion, to one, several or
possibly no judgements above. They represent requirements to conclude the below
part. In this case, one doesn't need to assume anything to conclude that \(A\)
entails \(A\).
Going back to conjunction, the rule\sidenote{It is actually one of many possible
rules. It all depends on the logic.} to prove one is the following.
\[
  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash B
    }
    {\Ga \vdash A \wedge B}
\]
That is to say, to prove \(A \wedge B\) (under hypotheses \(\Ga\)), it
\emph{suffices} to prove \(A\) and \(B\) (under the same hypotheses).
This is called an \emph{introduction} rule because it allows us to introduce a
connective in the conclusion.
Sometimes, we want to be able to conclude something from a complex assumption
like \(A \wedge B\), this is instead called an \emph{elimination} rule.
\begin{mathpar}
  \infer
    {\Ga, A, B \vdash P}
    {\Ga, A \wedge B \vdash P}
  %
\end{mathpar}
To prove \(P\) assuming \(A \wedge B\), it is enough to prove \(P\) assuming
both \(A\) and \(B\).
We then have similar rules for disjunction.
\marginnote[0.5cm]{
  To prove \(A \vee B\) you only need to prove either \(A\) or \(B\), hence the
  two introduction rules. On the contrary if you want to prove \(P\) assuming
  \(A \vee B\), you have to provide a proof for the two different cases: either
  \(A\) holds, or \(B\) holds, in both instances \(P\) should hold.
}
\begin{mathpar}
  \infer
    {\Ga \vdash A}
    {\Ga \vdash A \vee B}
  %

  \infer
    {\Ga \vdash B}
    {\Ga \vdash A \vee B}
  %

  \infer
    {
      \Ga, A \vdash P \\
      \Ga, B \vdash P
    }
    {\Ga, A \vee B \vdash P}
  %
\end{mathpar}

Amongst the most important rules are those related to implication.
\begin{mathpar}
  \infer
    {\Ga, A \vdash B}
    {\Ga \vdash A \to B}
  %

  \infer
    {
      \Ga \vdash A \to B \\
      \Ga \vdash A
    }
    {\Ga \vdash B}
  %
\end{mathpar}
They are interesting because they show a level of interaction between the
entailement and the implication. To prove that \(A \to B\), you only assume
\(A\) and show \(B\). Moreover, if you know \(A \to B\) and \(A\), then you
have \(B\). This last rule is called the \emph{modus ponens}.

Because of this, it might seem like implication (\(\to\)), entailement
(\(\vdash\)) and inference (---) are three forms of logical implication.
They \emph{are}, but at different levels.

\section{Proof frameworks}

There are many proof frameworks and logics and I will not review all of them,
but I will present the most relevant to the rest of my work.

\subsection{Classical logic}

Classical logic is the logic most people have in mind where any proposition is
either ``true'' or ``false'' (but not either provable or provably
contradictory). Gentzen describe it in a sequent calculus called
\LK~\sidecite{gentzen1935untersuchungen}.

The syntax is given by
\[
  \begin{array}{rrl}
    P, Q, A, B &\bnf& \top \bnfor \bot \bnfor P \wedge Q \bnfor P \vee Q
    \bnfor P \to Q \bnfor \neg P \\
    \Ga, \D &\bnf& \ctxempty \bnfor \Ga, P \\
    \mathcal{J} &\bnf& \Ga \vdash \D
  \end{array}
\]
which corresponds to \emph{propositional logic}.
I will give the rules in separate bundles: the first of which consists of
the two important rules of \emph{axiom} and \emph{cut}.
\marginnote[1cm]{
  Next to the rules I write their name in parentheses.
  \(I\) stands for identity.
}
\begin{mathpar}
  \infer
    { }
    {A \vdash A}
  (I)

  \infer
    {
      \Ga \vdash \D, A \\
      A, \Xi \vdash \Theta
    }
    {\Ga, \Xi \vdash \D, \Theta}
  (Cut)
\end{mathpar}
While the \emph{axiom} rule has nothing special about it, the \emph{cut} rule
might seem a bit peculiar. If we forget about \(\D\) and \(\Xi\) it says that if
\(\Ga\) entails \(A\) which itself entails \(\Theta\) then \(\Ga\) entails
\(\Theta\), which is not so far from the \emph{modus ponens} after all.

Then we have the logical rules---\ie that pertain to the logical constructors
of propositional logic:
\marginnote[1cm]{
  The \(L\)s and \(R\)s in rules indicate whether it operates on the left or
  right of the sequent.
}
\begin{mathpar}
  \infer
    {\Ga, A \vdash \D}
    {\Ga, A \wedge B \vdash \D}
  (\wedge L_1)

  \infer
    {\Ga \vdash A, \D}
    {\Ga \vdash A \vee B, \D}
  (\vee R_1)

  \infer
    {\Ga,B \vdash \D}
    {\Ga, A \wedge B \vdash \D}
  (\wedge L_2)

  \infer
    {\Ga \vdash B, \D}
    {\Ga \vdash A \vee B, \D}
  (\vee R_2)

  \infer
    {
      \Ga, A \vdash \D \\
      \Xi, B \vdash \Theta
    }
    {\Ga, \Xi, A \vee B \vdash \D, \Theta}
  (\vee L)

  \infer
    {
      \Ga \vdash A,\D \\
      \Xi \vdash B, \Theta
    }
    {\Ga, \Xi \vdash A \wedge B, \D, \Theta}
  (\wedge R)

  \infer
    {
      \Ga \vdash A, \D \\
      \Xi, B \vdash \Theta
    }
    {\Ga, \Xi, A \to B \vdash \D, \Theta}
  (\to L)

  \infer
    {\Ga, A \vdash B, \D}
    {\Ga \vdash A \to B, \D}
  (\to R)

  \infer
    {\Ga \vdash A, \D}
    {\Ga, \neg A \vdash \D}
  (\neg L)

  \infer
    {\Ga, A \vdash \D}
    {\Ga \vdash \neg A, \D}
  (\neg R)

  \infer
    {\Ga \vdash \D}
    {\Ga, \top \vdash \D}
  (\top L)

  \infer
    { }
    {\Ga \vdash \top, \D}
  (\top R_1)

  \infer
    { }
    {\vdash \top}
  (\top R_2)

  \infer
    { }
    {\bot \vdash}
  (\bot L_1)

  \infer
    { }
    {\Ga, \bot \vdash \D}
  (\bot L_2)

  \infer
    {\Ga \vdash \D}
    {\Ga \vdash \bot, \D}
  (\bot R)
\end{mathpar}
\marginnote[-2.5cm]{
  \(\bot L_2\) and \(\top R_1\) are redundant with weakening and \(\bot L_1\)
  and \(\top R_2\) but I keep them as weakening is sometimes omitted from the
  structural rules.
}

The rest are structural rules that allow us to weaken the hypotheses or the
conclusion, or reorder propositions and remove duplicates.
\marginnote[1cm]{
  \(W\) stands for weakening, \(C\) for contraction and \(P\) for permutation.
}
\begin{mathpar}
  \infer
    {\Ga \vdash \D}
    {\Ga, A \vdash \D}
  (WL)

  \infer
    {\Ga \vdash \D}
    {\Ga \vdash A, \D}
  (WR)

  \infer
    {\Ga, A, A \vdash \D}
    {\Ga, A \vdash \D}
  (CL)

  \infer
    {\Ga \vdash A, A, \D}
    {\Ga \vdash A, \D}
  (CR)

  \infer
    {\Ga, A, B, \Xi \vdash \D}
    {\Ga, B, A, \Xi \vdash \D}
  (PL)

  \infer
    {\Ga \vdash \D, A, B, \Theta}
    {\Ga \vdash \D, B, A, \Theta}
  (PR)
\end{mathpar}

\LK is classical because we can derive the \acrlong{LEM} from it.
\begin{mathpar}
  \infer*[Right=\((CR)\)]
    {
      \infer*[Right=\((\vee R_1)\)]
        {
          \infer*[Right=\((PR)\)]
            {
              \infer*[Right=\((\vee R_2)\)]
                {
                  \infer*[Right=\((\neg R)\)]
                    {
                      \infer*[Right=\((I)\)]
                        { }
                        {P \vdash P}
                    }
                    {\vdash \neg P, P}
                }
                {\vdash P \vee \neg P, P}
            }
            {\vdash P, P \vee \neg P}
        }
        {\vdash P \vee \neg P, P \vee \neg P}
    }
    {\vdash P \vee \neg P}
\end{mathpar}
\LK also features double negation elimination:
\[
  \infer*[Right=\((\to R)\)]
    {
      \infer*[Right=\((\neg L)\)]
        {
          \infer*[Right=\((\neg R)\)]
            {
              \infer*[Right=\((I)\)]{ }{P \vdash P}
            }
            {\vdash \neg P, P}
        }
        {\neg \neg P \vdash P}
    }
    {\vdash \neg \neg P \to P}
\]

\subsection{Intuitionistic logic}

Intuitionistic logic doesn't abide by the \acrshort{LEM}, so its treatment of
negation is a bit different so that double negation cannot be eliminated as
above. As such we define \LJ in natural deduction (the sequent style was also
of import to get classical logic), on the same syntax of propositional logic.
It is pretty similar.
\begin{mathpar}
  \infer
    { }
    {A \vdash A}
  (I)

  \infer
    {
      \Ga \vdash A \\
      A, \D \vdash B
    }
    {\Ga, \D \vdash B}
  (Cut)
\end{mathpar}

We have less rules because some don't make sense anymore with only one
proposition on the right.
\begin{mathpar}
  \infer
    {\Ga, A, B \vdash C}
    {\Ga, A \wedge B \vdash C}
  (\wedge L)

  \infer
    {\Ga \vdash A}
    {\Ga \vdash A \vee B}
  (\vee R_1)

  \infer
    {\Ga \vdash B, \D}
    {\Ga \vdash A \vee B, \D}
  (\vee R_2)

  \infer
    {
      \Ga, A \vdash C \\
      \Ga, B \vdash C
    }
    {\Ga, A \vee B \vdash C}
  (\vee L)

  \infer
    {
      \Ga \vdash A \\
      \Ga \vdash B
    }
    {\Ga \vdash A \wedge B}
  (\wedge R)

  \infer
    {
      \Ga \vdash A \\
      \D, B \vdash C
    }
    {\Ga, \D, A \to B \vdash C}
  (\to L)

  \infer
    {\Ga, A \vdash B}
    {\Ga \vdash A \to B}
  (\to R)

  \infer
    {\Ga \vdash A}
    {\Ga, \neg A \vdash B}
  (\neg L)

  \infer
    {\Ga, A \vdash \bot}
    {\Ga \vdash \neg A}
  (\neg R)

  \infer
    {\Ga \vdash A}
    {\Ga, \top \vdash A}
  (\top L)

  \infer
    { }
    {\vdash \top}
  (\top R)

  \infer
    { }
    {\Ga, \bot \vdash A}
  (\bot L)
\end{mathpar}

The structural rules are only happening on the left now since there is nothing
to rearrange on the right.
\begin{mathpar}
  \infer
    {\Ga \vdash B}
    {\Ga, A \vdash B}
  (W)

  \infer
    {\Ga, A, A \vdash B}
    {\Ga, A \vdash B}
  (C)

  \infer
    {\Ga, A, B, \D \vdash A}
    {\Ga, B, A, \D \vdash A}
  (P)
\end{mathpar}

This logic is constructive in that from every proof derivation one can extract
a proof where the last rule is an introduction rule, \ie a \emph{right} rule.
This is obtained by a process known as \emph{cut-elimination} which removes all
occurrences of the \((Cut)\) rule in a derivation, showing at the same time that
it is \emph{admissible} or redundant.

\subsection{Mechanised proofs}
\todo{mechanised proofs, proof assistants, automated theorem provers}