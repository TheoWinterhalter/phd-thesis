% \setchapterpreamble[u]{\margintoc}
\chapter{Dependent types}
\labch{dependent-types}

The idea behind dependent types is that they now can \emph{depend} on terms,
\ie terms can appear in types.
\marginnote[1cm]{
  Thus, \(x = y\) becomes a type depending on \(x\) and \(y\), and whose
  inhabitants are witnesses of the fact that \(x\) and \(y\) are equal.
}
This is very interesting because we can talk about things like \(P\ n\) for a
property \(P\) on a natural number \(n\), equality \(x = y\) for two terms \(x\)
and \(y\), and with it, we can support quantifiers.

It is not only useful on the logical side, but also on the programming language
side: with it programs can be given much more precise types.
For instance, you might want to specify that the division operator does not
accept \(0\) for the denominator, or that the operation returning the tail of
list only applies to non-empty lists.
You can also have the type of lists of length \(n\).
Finally, we can take advantage of both and write \emph{proofs} about the
programs we wrote, using the very same language.

\section{A minimal dependent type theory}

Let me describe a very basic type theory featuring dependent types.

\paradot{\(\Pi\)-types}

The simplest way to get dependent types is to extend the \acrshort{STL}---which
only features arrow, or function, types---with dependent function types or
\(\Pi\)-types.
\[
  \infer
    {\Ga, x :A \vdash t : B}
    {\Ga \vdash \lambda (x:A).t : \Pi (x:A).\ B}
  %
\]
As you can see, we keep the \(\lambda\)-terms of earlier but now they represent
dependent functions. Now, not only \(t\) can mention \(x\), but \(B\) also.
\marginnote[-1.3cm]{
  Polymorphism is the ability to use a program at different types. The
  polymorphic identity function means that it is the identity function for any
  type that we desire: we can type it with \(\mathbb{N} \to \mathbb{N}\),
  \(\bool \to \bool\) or anything really.
}
For instance we can write the polymorphic identity function as follows
\marginnote[0.8cm]{
  I will explain later what the \(\Type\) here means, but it should be
  intuitive: I am taking a \emph{type} \(A\) as argument and then an element
  \(a\) of that type.
}
\[
  \lambda (A : \Type).\ \lambda (a : A).\ a
\]
and it has type
\[
  \Pi (A : \Type).\ \Pi (a : A).\ A
\]
a fact that we can write
\[
  \lambda\ (A : \Type)\ (a : A).\ a
  : \Pi\ (A : \Type)\ (a : A).\ A
\]
or even more concisely as
\marginnote[0.5cm]{
  \(A \to B\) is no longer a definition, but a notation for the dependent
  function type which is not actually dependent \(\Pi (\_ : A).\ B\).
}
\[
  \lambda\ (A : \Type)\ (a : A).\ a
  : \Pi\ (A : \Type).\ A \to A
\]
since \(a\) is not mentioned in the type.
Now we should be able to see the dependency on \(A\).
One can argue that \(A\) is a \emph{type} and not a term, but in this setting,
types are just a special kind of terms that happen to be of type \(\Type\).

We will also be able to write functions like
\[
  \lambda (n : \mathbb{N}).\ [ 0, 1, \dots, n ] :
  \Pi (n : \mathbb{N}).\ \tvec\ (n+1)
\]
where \(\tvec\ n\) is the type of lists of length \(n\) containing natural
numbers (which we call vectors).

If we have some \(B : \Type\) we might want to apply our polymorphic identity
function to it to get the identity function on \(B\), \ie
\[
  \lambda (a:B).\ a : B \to B
\]

For this we have to rely on substitutions again, not only in the terms after
\(\beta\)-reduction, but also in types.
This can be seen in the application rule:
\[
  \infer
    {
      \Ga \vdash u : \Pi (x:A).\ B \\
      \Ga \vdash v : A
    }
    {\Ga \vdash u\ v : B[x \sto v]}
  %
\]
Once again it is pretty similar to the application rule of simple type theory
except we have to account for the dependency. In our example---writing \(\cid\)
for the polymorphic identity function---we have
\marginnote[0.7cm]{
  What happens is the type of the application is
  \[
    (A \to A)[A \sto B] = B \to B
  \]
}
\[
  \infer
    {
      \Ga \vdash \cid : \Pi (A:\Type).\ A \to A \\
      \Ga \vdash B : \Type
    }
    {\Ga \vdash \cid\ B : B \to B}
  %
\]
We can apply our vector function to \(3\) for instance to get
\[
  (\lambda (n : \mathbb{N}).\ [ 0, 1, \dots, n ])\ 3 : \tvec\ 4
\]
The result of this application (\ie after \(\beta\)-reduction) is
\[
  [0, 1, 2, 3]
\]
which is indeed a list of length \(4\).

\paradot{Universes}

In the example above there is the peculiar type \(\Type\).
This can be thought of as the type of types. If you know about Russell's paradox
stating that there can be no set of all sets, you might be skeptical and indeed
having \(\Type\) of type \(\Type\) is inconsistent.
We will address this in more details in \arefsubsec{coq-univ} in
\nrefch{flavours}.
In this case, \(\Type\) will be a special type---which we call \emph{universe}
as it is inhabited solely by types---that does not have a type itself.
Having it is mainly to allow for quantifying over types, but we could also of
course define some base types like the natural numbers \(\nat\) and put it in
\[
  \nat : \Type
\]
We will see more example of types in \nrefch{usual-defs}. For now we only have
\(\Pi\)-types in them:
\[
  \infer
    {
      \Ga \vdash A : \Type \\
      \Ga, x : A \vdash B : \Type
    }
    {\Ga \vdash \Pi (x:A).\ B : \Type}
  %
\]
Here we evidence the fact \(B\) is indeed dependent over \(x : A\)
and that the type \(\Pi (x:A).\ B\) is correctly constructed.
This kind of universe is called a Russell universe, and there are also Tarski
universes, the difference will be explained in \arefsubsec{univ-and-types}
of \nrefch{formalisation}.

\paradot{Computation}
As was the case in \nrefch{simple-types}, we have \(\beta\)-reduction in
dependent type theory:
\[
  (\lambda (x:A).\ t)\ u \red_\beta t[x \sto u]
\]
Now that terms can appear in types, reduction can also happen in them.
For instance, we would probably want \(\tvec\ (3 + 1)\) and \(\tvec\ 4\) to be
related. We will in fact have
\[
  \tvec\ (3 + 1) \red \tvec\ 4
\]
If you consider the following \(\mathsf{concat}\) function with concatenates
two vectors
\[
  \mathsf{concat} :
  \Pi\ (n : \mathbb{N})\ (m : \mathbb{N})\ (v_1 : \tvec\ n)\ (v_2 : \tvec\ m).\
  \tvec\ (n + m)
\]
we would have
\[
  \mathsf{concat}\ 3\ 1\ [0,1,2]\ [3] : \tvec\ (3 + 1)
\]
meaning that \(\tvec\ (3 + 1)\) can appear naturally.
In this case, we expect the concatenation function to compute like this
\[
  \mathsf{concat}\ 3\ 1\ [0,1,2]\ [3] \red [0,1,2,3]
\]
but as I presented before, \([0,1,2,3]\) has type \(\tvec\ 4\).
As such we want to argue that \(\tvec\ (3 + 1)\) and \(\tvec\ 4\) are the same.
For this we introduce the notion of \emph{conversion}.
Conversion \(A \equiv B\) essentially means that \(A\) and \(B\) are equal up
to reduction (\(\red\)); we say that \(A\) and \(B\) are \emph{convertible}.
In our case we would have
\[
  \tvec\ (3 + 1) \equiv \tvec\ 4
\]
We can change a type for a convertible one in typing, essentially saying that if
\(t\) has type \(A\) and \(A\) is convertible to \(B\) then \(t\) also has type
\(B\):
\[
  \infer
    {
      \Ga \vdash t : A \\
      A \equiv B
    }
    {\Ga \vdash t : B}
  %
\]
This allows us to say
\[
  \mathsf{concat}\ 3\ 1\ [0,1,2]\ [3] : \tvec\ 4
\]
directly.

\marginnote[0.1cm]{
  See \nrefch{desirable-props} for a definition of these properties.
}
It is often the case that reduction (\(\red\)) is confluent and terminating and
that, as such, conversion is decidable.


I will now put the syntax and the rules together for clarity, at the risk of
repeating myself.
\marginnote[1.2cm]{
  \(\ctxempty\) is the empty context.
}
\[
  \begin{array}{rcl}
    A, B, t, u &\bnf& x \bnfor \lambda (x:A).t \bnfor t\ u \bnfor \Pi (x:A). B
    \bnfor \Type \\
    \Ga, \D &\bnf& \ctxempty \bnfor \Ga, x:A
  \end{array}
\]

\begin{mathpar}
  \infer
    {(x : A) \in \Ga}
    {\Ga \vdash x : A}
  %

  \infer
    {
      \Ga \vdash A : \Type \\
      \Ga, x : A \vdash B : \Type
    }
    {\Ga \vdash \Pi (x:A).\ B : \Type}
  %

  \infer
    {
      \Ga, x :A \vdash t : B \\
      \Ga \vdash \Pi (x:A).\ B : \Type
    }
    {\Ga \vdash \lambda (x:A).t : \Pi (x:A).\ B}
  %

  \infer
    {
      \Ga \vdash u : \Pi (x:A).\ B \\
      \Ga \vdash v : A
    }
    {\Ga \vdash u\ v : B[x \sto v]}
  %

  \infer
    {
      \Ga \vdash t : A \\
      A \equiv B \\
      \Ga \vdash B : \Type
    }
    {\Ga \vdash t : B}
  %
\end{mathpar}
\marginnote[-4.5cm]{
  Here I changed a bit the typing rule for \(\lambda\)-abstraction to also ask
  for its type to be well-formed. This is necessary to ensure we put legitimate
  types in the context.
}%
\marginnote[-1.5cm]{
  Similarly, I ask that the type \(B\) is well-typed in the conversion rule.
}

Where conversion is defined as the congruent closure of \(\beta\)-reduction.
\begin{mathpar}
  \infer
    { }
    {t \equiv t}
  %

  \infer
    {u \equiv v}
    {v \equiv u}
  %

  \infer
    {
      u \equiv v \\
      v \equiv w
    }
    {u \equiv w}
  %

  \highlight{
    \infer
      { }
      {(\lambda (x:A).\ t)\ u \equiv t[x \sto u]}
    %
  }

  \infer
    {
      A \equiv A' \\
      B \equiv B'
    }
    {\Pi (x:A).B \equiv \Pi (x:A').B'}
  %

  \infer
    {
      A \equiv A' \\
      t \equiv t'
    }
    {\lambda (x:A).t \equiv \lambda (x:A').t'}
  %

  \infer
    {
      u \equiv u' \\
      v \equiv v'
    }
    {u\ v \equiv u'\ v'}
  %
\end{mathpar}

We usually also add a definition of well-formed contexts to ensure they are
comprised of types that make sense and that the dependencies are in order:
\marginnote[1cm]{
  As you can see, each type might depend on the previous variables.
}
\begin{mathpar}
  \infer
    { }
    {\vdash \ctxempty}
  %

  \infer
    {
      \vdash \Ga \\
      \Ga \vdash A : \Type
    }
    {\vdash \Ga, x:A}
  %
\end{mathpar}

\section{Dependent types in \Coq}

In this thesis I will often refer to the type theory of \Coq as well as give
some definitions in it. I will give here a brief introduction to it with
examples, but this does not aim at being a tutorial or a manual for \Coq.

In \Coq, \(\Pi\)-types are written
\begin{minted}{coq}
forall (x : A), B
\end{minted}
and \(\lambda\)-abstractions are written
\begin{minted}{coq}
fun (x : A) => t
\end{minted}
In both cases the domain (\mintinline{coq}{A}) can be left out if \Coq manages
to infer it from the context:
\marginnote[0.4cm]{
  \mintinline{coq}{nat} is the built-in type of natural numbers.
}
\begin{minted}{coq}
fun x => x + 0 : nat -> nat
\end{minted}

The polymorphic identity function of earlier is
\begin{minted}{coq}
fun A x => x : forall A, A -> A
\end{minted}
We can write it as a definition in the system as follows:
\begin{minted}{coq}
Definition id {A : Type} (x : A) : A := x.
\end{minted}
\marginnote[-0.6cm]{
  The squigly brackets indicate that the argument \mintinline{coq}{A} is
  implicit so that we can later write \mintinline{coq}{id 0} so that \Coq infers
  that \mintinline{coq}{A} is \mintinline{coq}{nat}.
}

Writing down functions is not the only way to define terms in \Coq however.
One of its strengths is the tactic mechanism that it is equipped with which
allows the user to write proofs in an interactive way.
Instead of writing the polymorphic identity function we could prove the
mathematical statement \(\forall A, A \to A\).
\begin{minted}{coq}
Fact id_as_proof :
  forall A, A -> A.
Proof.
  intro A.
  intro x.
  assumption.
Qed.
\end{minted}
The way this works is that we first state what we wish to prove
\begin{minted}{coq}
forall A, A -> A
\end{minted}
and how we want to refer to that fact afterwards
(\mintinline{coq}{id_as_proof}).
After the keyword \mintinline{coq}{Proof}, \Coq is in interactive mode,
telling the user what they have to prove to conclude the proof.
At the beginning this is still the full statement
\marginnote[0.4cm]{
  The horizontal bar separates the hypotheses (above) from the goal to prove
  (below).
}
\begin{minted}{coq}
-----------------
forall A, A -> A
\end{minted}
The user can then write tactics to progress with the proof.
I will write side by side the tactics used to progress and the goal after its
execution.

To prove a quantified statement, as usual, you want to assume some element in
particular and prove the statement for that one. This is what the tactic
\mintinline{coq}{intro} does.

\begingroup
\centering
\begin{tabular}{p{0.5\textwidth}|p{0.5\textwidth}}
\begin{minted}{coq}
intro A.
\end{minted}
&
\begin{minted}{coq}
A : Type
-----------------
A -> A
\end{minted}
\end{tabular}
\endgroup

We have now \emph{introduced} \mintinline{coq}{A} in our context and need to
prove \mintinline{coq}{A -> A}.

\begingroup
\centering
\begin{tabular}{p{0.5\textwidth}|p{0.5\textwidth}}
\begin{minted}{coq}
intro A.
intro x.
\end{minted}
&
\begin{minted}{coq}
A : Type
x : A
-----------------
A
\end{minted}
\end{tabular}
\endgroup

We assumed some \mintinline{coq}{x : A}, so it joined the assumptions, leaving
us to prove \mintinline{coq}{A}.
Proving \mintinline{coq}{A}, when we have \mintinline{coq}{A} as an hypothesis
should be pretty straightforward. The tatic \mintinline{coq}{assumption}
tells \Coq that the goal can be concluded using one of the assumptions in the
context (here \mintinline{coq}{x} since \mintinline{coq}{x : A} \ie
\mintinline{coq}{x} is a proof witness of \mintinline{coq}{A}).

\begingroup
\centering
\begin{tabular}{p{0.5\textwidth}|p{0.5\textwidth}}
\begin{minted}{coq}
intro A.
intro x.
assumption.
\end{minted}
&
\begin{minted}{coq}
No more goals.
\end{minted}
\end{tabular}
\endgroup

This generates a term, and when we write \mintinline{coq}{Qed} it is checked
by \Coq to make sure it is correct.

As one should expect this produces the same term as before
\begin{minted}{coq}
id_as_proof = fun A x => x
            : forall A, A -> A
\end{minted}
That is to say, the polymorphic identity function is a proof witness of
\begin{minted}{coq}
forall A, A -> A
\end{minted}

There is of course much more to it and \nrefch{usual-defs} will offer some
usual definitions in type theory and in the context of \Coq.