% \setchapterpreamble[u]{\margintoc}
\chapter{Flavours of type theory}
\labch{flavours}

Type theory comes in many different flavours and shapes, different formulations
and properties. I will not try to be exhaustive but I will try to cover the
main kind of dependent type theories I have encountered.
I will not attempt to define properly the notion of \emph{type theory},
there is work on this~\misref{} but it is still a bit early to grasp the
concept fully.

\section{Computation and type theory}

The first prism in which to see type theory through can be that of computation.
Indeed, not all type theories feature it to the same extent. Though for some
people in computation resides the essence of type theory, it is still worth
it to investigate theories where conversion is defined differently.

\subsection{Intensional Type Theory}
\labsubsec{pts-itt}

\acrfull{ITT} is the name given to a wide range of type theories actually.
Those could be described in the setting of \acrfullpl{PTS}.
The theories behind the proof assistants \Coq and
\Agda~\sidecite[-0.4cm]{norell2007towards}--respectively \acrfull{PCUIC} and
\acrfull{MLTT}\sidenote{Note that \acrshort{MLTT} also has various
forms.}---are variants of \acrshort{ITT}.

A \acrshort{PTS} is a pretty basic type theory, it is parametrised by a
collection of sorts \cS, with so called \emph{rules}
(\Rl) and \emph{axioms} (\Ax).
Its syntax features \(\lambda\)-abstractions, applications, variables,
\(\Pi\)-types and sorts.
%
\[
  \begin{array}{lrl}
    s &\in& \mathcal{S} \\
    T,A,B,t,u,v &\bnf& x \bnfor \lambda (x:A). t \bnfor t\ u
    \bnfor \Pi (x : A). B \bnfor s \\
    \Ga, \D &\bnf& \ctxempty \bnfor \Ga, x:A
  \end{array}
\]

Their computational behaviour is defined by a reduction relation (\(\red\))
which is the contextual closure of the \(\beta\)-reduction.
\[
  (\lambda (x:A). t)\ u \red_\beta t[x \sto u]
\]
\marginnote[-0.9cm]{
  For instance
  \(\lambda (x : A). (\lambda (y : B) y x)\ t \red \lambda (x : A). t x\)
}

The typing rules involve the rules and axioms we mentioned earlier.
%
\begin{mathpar}
  \infer
    {(s,s') \in \Ax}
    {\isterm{\Ga}{s}{s'}}
  %

  \infer
    {
      \isterm{\Ga}{A}{s_1} \\
      \isterm{\Ga, x : A}{B}{s_2} \\
      (s_1, s_2, s_3) \in \Rl
    }
    {\isterm{\Ga}{\Pi (x:A). B}{s_3}}
  %

  \infer
    {(x : A) \in \Ga}
    {\isterm{\Ga}{x}{A}}
  %

  \infer
    {
      \isterm{\Ga, x:A}{t}{B} \\
      \isterm{\Ga}{\Pi (x:A).B}{s}
    }
    {\isterm{\Ga}{\lambda (x:A).t}{\Pi (x:A).B}}
  %

  \infer
    {
      \isterm{\Ga}{t}{\Pi (x:A). B} \\
      \isterm{\Ga}{u}{A}
    }
    {\isterm{\Ga}{t\ u}{B[x \sto u]}}
  %

  \infer
    {
      \isterm{\Ga}{t}{A} \\
      A \equiv B \\
      \isterm{\Ga}{B}{s}
    }
    {\isterm{\Ga}{t}{B}}
  %
\end{mathpar}
%
Axioms determine typing of sorts, and rules what dependent products are allowed.
The last rule is the conversion rule, it is the rule that involves computation:
basically you can exchange two computationally equal types in a typing
judgement. The \(\isterm{\Ga}{B}{s}\) bit is to make sure the type we want
to substitute still makes sense.
In this case, conversion (\(\equiv\)) is defined as the reflexivie,
symmetric, transitive closure of reduction.
\marginnote[-0.4cm]{
  \(t \equiv u\) is defined as \(t \mathop{(\redl . \red)^\star} u\)
}

\marginnote[0.5cm]{
  See \nrefch{usual-defs}.
}
When talking about \acrshort{ITT} we usually mean an extension of this with
more concepts like some base (inductive) types and computation rules on their
eliminators (pattern-matching). The conversion rule is also not always strictly
derived from the reduction alone, it often includes \(\eta\)-rules, the most
common being \(\eta\)-expansion of functions.
\[
  f \equiv_\eta \lambda (x:A). f\ x
\]
For it to make sense, expansion has to be limited to functions which require
type information\sidenote{Also, the \(A\)---domain of the function---has to
be sumrised somehow.}, this is why in certain contexts---like \Agda---the
conversion is also typed.
The relation between typed and untyped conversion has been explored at several
occasions~\sidecite[1.8cm]{van2013explicit}.
\Coq manages to verify \(\eta\)-conversion for functions and records without
relying on a typed-conversion as we will see later~\misref.
\marginnote[-0.5cm]{
  For instance, \(\eta\) for pairs is \(p \equiv (p.1, p.2)\) where
  \(p.1\) and \(p.2\) are the first and second projections of \(p\).
}

\subsection{Extensional Type Theory}
\labsubsec{ett-def}

\acrfull{ETT} is an extension of \acrshort{ITT} where conversion is extended
to capture all provable equalities, this principle is called \emph{reflection
(of equality)}.
This of course implies that the considered \acrshort{ITT} is equipped with
an equality type.

\marginnote[1cm]{
  Here I use \(\vdash_\exmark\) to mark that this is a judgement in
  \acrshort{ETT}.
}
\begin{definition}{Reflection Rule}
  \labdef{reflection}
  \begin{mathpar}
    \infer[]
      {\xisterm{\Ga}{e}{\Eq{A}{u}{v}}}
      {\xeqterm{\Ga}{u}{v}{A}}
    %
  \end{mathpar}
\end{definition}

As you can see, this time I opted for a typed conversion, I think it makes more
sense since the conversion is more semantical than syntactical.
Also, you can note the little \(\exmark\) subscript, its purpose is to mark
the judgement as beeing \emph{ex}tensional.
\Andromeda and \NuPRL implement variants of
\acrlongpl{ETT}~\sidecite{andromeda,nuprl}.
To see its usefulness, we're going to look at the definition of reversal of
vectors in \Coq, using an accumulator for the definiton to be tail-recursive.
%
\begin{minted}{coq}
Definition vrev {A n m} (v : vec A n) (acc : vec A m)
  : vec A (n + m) :=
  match v with
  | vnil => acc
  | vcons a n v => vrev v (vcons a m acc)
  end.
\end{minted}
%
The recursive call of \mintinline{coq}{vrev} returns a vector of length
\mintinline{coq}|n + S m| where the context expects one of length
\mintinline{coq}|S n + m|. In \acrshort{ITT} and \Coq, thesec types are not
convertible, and thus the definition is not accepted, even though it feels like
it is the right definition. \acrshort{ETT} solves this problem by exploiting
the fact that \mintinline{coq}|n + S m = S n + m| is provable.
You can still define it in \Coq, but you have to explicitely transport along
the abovementioned equality which can result in some problems while reasoning
on the resulting function and inconveniences overall.

\acrshort{ETT} is not the ultimate solution however and suffers from many
drawbacks the main of which being that type-checking is not decidable as we
shall see in \nrefch{elim-reflection-intro}.
We will also explore the relation between \acrshort{ETT} and \acrshort{ITT} in
\arefpart{elim-reflection}.

\subsection{Weak Type Theory}
\labsubsec{wtt}

A \acrfull{WTT} is on the other end of the spectrum: instead of extending
conversion with everything that can be proven equal, conversion is removed
altogether. Computation (like \(\beta\)-reduction) is now handled by
propositional equality alone, and conversion of types is done using transports
along said equality.

\begin{mathpar}
  \infer
    {
      \Ga \vdash A : s \\
      \Ga, x:A \vdash t : B \\
      \Ga \vdash u : A
    }
    {
      \Ga \vdash \beta(t,u) :
      (\lambda (x : A).\ t)\ u =_{B[x \sto u]} t[x \sto u]
    }
  %

  \infer
    {
      \Ga \vdash T_1 : s \\
      \Ga \vdash T_2 : s \\
      \Ga \vdash p : T_1 = T_2 \\
      \Ga \vdash t : T_1
    }
    {\Ga \vdash \transport{T_1}{T_2}{p}{t} : T_2}
  %
\end{mathpar}

This time it is a bit hard to advertise it for practical use in a proof
assistant, it is nonetheless interesting. For one, its meta-theory is that much
simpler, an even more attractive fact once combined with a translation from
\acrshort{ITT} (or \acrshort{ETT}) to \acrshort{WTT} as is the object of
\arefpart{elim-reflection}.
Another point worth mentioning is that it really crystallises the notion that
proofs are really just terms and do not require extra machinery to make sure
they are indeed proofs (even when conversion is decidable, it might takes
eons before you get this knowlegde)~\sidecite{de1991plea}.

One might also be tempted to call it minimal, but in order to simulate the
congruence aspect of conversion, we have to extend the theory with principles
to allow equalities under binders, and this has to be done for each binder
(once for \(\lambda\)-abstractions---the usual functional extensionality---and
once for \(\Pi\)-types---much less standard---at least).

\section{Focus on the theory behind \Coq}
\labsec{coq-theory}

\Coq is originally based on the \acrfull{CoC}, or the
\acrfull{CIC}~\sidecite{bertot2004interactive}, though it is nowadays rather
called the \acrfull{PCUIC}.
It is a variant of \acrshort{ITT} but as the name suggests, it has several
extra features, hinted at by the words \emph{cumulative}, \emph{inductive}
and \emph{predicative}.

\subsection{Universes in \Coq}
\labsubsec{coq-univ}

\Coq features a \emph{predicative} hierarchy of universes
\((\Type_i)_{i \in \mathbb{N}}\) such that \(\Type_i : \Type_j\) for any
\(i < j\). The presence of several universes included in one another is not
there for fun, it is there to circumvent Russell's paradox which
shows it is inconsistent to have \(\Type : \Type\).
\marginnote[-1cm]{
  For type theory we would more likely talk about Girard's
  paradox~\sidecite[1cm]{girard1972interpretation} or Hurkens'
  paradox~\sidecite[1.7cm]{hurkens1995simplification}.
}
The \Coq user however does not usually has to deal with those and
instead rely on the so-called
% \emph{typical ambiguity}~\sidecite[1.5cm]{harper1991type,feferman2004typical,specker1966typical}:
\emph{typical ambiguity}~\sidecite[1.5cm]{harper1991type}:
in the \Coq proof assistant one will simply write \(\Type\) and \Coq will infer
constraints to know whether it is possible to appoint each occurrence of
\(\Type\) a \emph{level}, \ie a natural number, such that the universes are used
consistently.

Alongside this hierarchy comes another universe, \(\Prop\) which is the universe
of propositions. This universe is \emph{impredicative}.
Impredicativity means that the definition of an object can quantify over the
object itself. In the case of \(\Prop\) it comes from the fact that a
proposition (\ie a type in \(\Prop\)) can quantify over propositions: \eg
\(\forall (P : \Prop).\ P\) is still a proposition.
\marginnote[-0.5cm]{
  % We write \(\forall (P : \Prop).\ P : \Prop\) to mean that the term
  % \(\forall (P : \Prop).\ P\) as type \(\Prop\).
  Note that the type \(\forall (P : \Prop).\ P\) is actually the empty type or
  false proposition so it is reassuring that it \emph{is} a proposition.
}
Actually impredicativity of \(\Prop\) is a bit stronger as it accepts \emph{any}
quantification, so that \(\forall (n : \mathbb{N}).\ n = n\) is also a
proposition for instance.
\(\Prop\) however is of type \(\Type_i\) for any \(i\).

Recently, a new universe of propositions has been
introduced~\sidecite{gilbert:hal-01859964}: \(\SProp\).
This time it is a universe of \emph{strict} propositions in the sense that any
two proofs of the same proposition are considered to be convertible.
\marginnote[-0.5cm]{
  For \(P : \SProp\) and \(p_1 : P\) and \(p_2 : P\) then \(p_1 \equiv p_2\).
}
In other words, there is no computational content to proofs of strict
propositions, and as such proofs are \emph{irrelevant}, only their mere
existence matters.
Although this addition is a really interesting subject, it goes a bit beyond
the scope of this thesis, in particular I do not consider it in my
formalisations (for now).

When I introduced the typing rules for \(\Prop\) and \(\Type\) I showed that
each universe can be typed in several other universes (actually in each case
there are an infinite number of possiblities). The reality is even more
intricate than that and it has to do with the word `\emph{cumulative}' found in
the name of the system.
\Coq features a notion of subtyping that is limited to universes and that is
called \emph{cumulativity}. In essence it says that \(\Type_i \le \Type_j\)
whenever \(i \le j\), that is that bigger universes contain the smaller
universes. Cumulativity is given by the following rules.
\marginnote[1.5cm]{
  Notice how the usual conversion rule is replaced by a cumulativity rule.
  Also, as opposed to usual subtyping definitions, there is no subtyping
  in contravariant positions, but rather conversion.
}
\begin{mathpar}
  \infer*[right=(\(i \le j\)), vcenter]
    { }
    {\Ga \vdash \Type_i \cumul \Type_j}
  %

  \infer
    { }
    {\Ga \vdash \Prop \cumul \Type_i}
  %

  \infer
    {
      \Ga \vdash A \equiv A' \\
      \Ga, x : A \vdash B \cumul B'
    }
    {\Ga \vdash \Pi\ (x : A).\ B \cumul \Pi\ (x : A').\ B'}
  %

  \infer
    {
      \Ga \vdash t : A \\
      \Ga \vdash A \cumul B \\
      \Ga \vdash B
    }
    {\Ga \vdash t : B}
  %
\end{mathpar}
This convenience allows the user to forget as much as possible about universes.


Finally there is another point regarding universes that is not hinted at in the
name: \emph{polymorphism}. This feature introduced
by~\sidecite{sozeau2014universe} is complementary to cumulativity and allows to
consider definitions that quantify over universe levels (the \(i\) in
\(\Type_i\)).
For instance, it allows to write a truly polymorphic identity function
\[
  \mathsf{id}_i \coloneqq \lambda (A : \Type_i)\ (x : A).\ x
\]
Here, the \(i\) subscript is to bind level \(i\) in the expression. Notice that
it is attached to the constant definition and not on the right-hand side; that's
because universe levels are only quantified in a prenex-style, meaning you
must quantify over \emph{all} universe levels before introducing other
variables. In particular there is no type of polymorphic
identities.

One might notice that currently, universe polymorphism only talks about
\(\Type\) and does not make mention of \(\Prop\). This is a shortcoming that
could be fixed using also \emph{sort polymorphism} but at the time of writing
of this document, another technology is used in place:
\emph{template polymorphism}. It is a heuristic orthogonal to universe
polymorphism to define constants both at the \(\Type\)-level and at the
\(\Prop\)-level at the same time. However, this seems to be the source of many
bugs and should be removed from \Coq's kernel at some point in the relatively
near future.

\subsection{Inductive types}

Then comes the \emph{inductive} part of the name. I already described
inductive types in \nrefsec{inductive-types} so I will not repeat myself
but their presence is the main justification for the `I' in \acrshort{CIC}.
However, \acrshort{PCUIC} is named that way because it features
\emph{cumulative inductive types}~\sidecite{timany2018cumulative}
(\acrshort{CIC}---and even \acrshort{CoC}---already has cumulativity).
The idea is to extend the cumulativity relation beyond sorts to inductive types
as well. I'm going to use the same example than the paper introducing them:
a record~\sidenote{Exploiting the fact that (non-primitive) records are actually
inductive types with one constructor in \Coq.} definition of categories:
\[
  \mathsf{Category}_{i,j} \coloneqq \{
    \mathsf{Obj} : \Type_i\ ;\
    \mathsf{Hom} : \mathsf{Obj} \to \mathsf{Obj} \to \Type_j\ ;\
    \dots
  \}
\]
In \acrshort{PCUIC}, you have the following cumulativity rule.
\begin{mathpar}
  \infer
    {
      i \le i' \\
      j \le j'
    }
    {\mathsf{Category}_{i,j} \cumul \mathsf{Category}_{i',j'}}
  %
\end{mathpar}
This allows for more flexibility when dealing with polymorphic definitions.

\subsection{let-bindings}

This is not limited to \Coq, but let-bindings behave differently than in
simply typed programming languages. In languages like \ocaml a let-binding can
be simulated by a \(\beta\)-redex.
\marginnote[0.5cm]{
  It might be that the compiler does things a bit differently in both cases
  for optimisations purposes, but at least \emph{in principle} they should be
  equivalent.
}
\begin{minted}{ocaml}
let x = u in t
\end{minted}
will be equivalent to
\begin{minted}{ocaml}
(fun x -> t) u
\end{minted}
as both will evaluate \mintinline{ocaml}{u} before substituting it for
\mintinline{ocaml}{x} in \mintinline{ocaml}{t}. This works mainly because
\ocaml is a \acrshort{CbV} language.
In \Coq, replacing the first by the second will not always yield a well-typed
expression.
\marginnote[0.5cm]{
  In \ocaml we have \(x : A \vdash t\).
}
In the \Coq expression \mintinline{coq}{let x := u in t}, \mintinline{coq}{t}
actually lives in an extended context with \mintinline{coq}{x := u}, meaning
that the definition is also stored, not just its type and name. This information
can be used to \emph{recall} that \mintinline{coq}{x} is convertible to
\mintinline{coq}{u}. The example below illustrates a case where the
transformation yields an ill-typed term.
\begin{minted}{coq}
Section Let.

  Context (u : nat).
  Context (f : u = u -> nat).

  Definition foo :=
    let x := u in f (eq_refl x).

  (** The term "eq_refl" has type "x = x" while it is expected
      to have type "u = u" (cannot unify "x" and "u").
  *)
  Fail Definition bar :=
    (fun x => f (eq_refl x)) u.

End Let.
\end{minted}

\section{Equality in type theories}

\nrefsec{equality-def} is already dedicated to the definition of equality in
type theory, but there I only presented \emph{one way} of dealing with equality,
the one that is prevalent in type theory based proof assistants like \Agda and
\Coq.

In \Coq equality is defined as follows.
\begin{minted}{coq}
Inductive eq (A : Type) (u : A) : A -> Prop :=
| eq_refl : eq A u u.
\end{minted}

This has some shortcomings\sidenote{At least for some people, many are content
with the current status too.} like equalities between equalities, and between
functions are not really accounted for, in the sense that's it is hard---often
impossible---to prove them.
\marginnote[0.2cm]{
  \acrshort{UIP} is given computational content in \Agda, it is feasible so
  it is not really an argument against this definition.
}
This gives rise to axioms like \acrshort{UIP} and \acrshort{funext}, but axioms
lack computational content.

Variants of type theory give different definitions and properties to equality.

\subsection{\acrfull{OTT}}

The idea behind \acrshort{OTT}~\sidecite{altenkirch2007observational} is that
equality should be defined for each type specifically and correspond to
observational equivalences. For functions, equality is \emph{defined} as
pointwise equality.
\[
  f =_{A \to B} g \coloneqq \Pi\ (x : A).\ f\ x =_B f\ y
\]
In this world, reflexivity becomes a property and is no longer part of the
definition.
More recently, this work has been extended to
\acrfull{STT}~\sidecite{altenkirch2019setoid} to support heterogenous equality
and better fit the Setoid model~\sidecite[0.3cm]{altenkirch1999extensional}.

\subsection{\acrfull{HoTT}}

\acrshort{HoTT}~\sidecite{hottbook} is rather based on the idea that
\acrshort{UIP} should not hold and that instead, higher equalities matter.
Some types still enjoy a certain form of \acrshort{UIP} however: they are called
sets, or homotopy sets (hSet). This actually defines a hierarchy of types, by
looking at their type of equality.

As the name suggests, equality is coming from an intuition of algebraic
topology: homotopy. An equality \(u = v\) is thus seen as a path connecting
\(u\) and \(v\).

\paradot{Univalence} One of the leading principles coming with \acrshort{HoTT}
is the univalence axiom. The idea is to lift the folklore idea in mathematics
that isomorphic objects are the \emph{same}, into a well-defined notion:
equivalent types are equal.
\marginnote[-0.5cm]{
  Equivalence \(A \cong B\) means roughly that there exist maps \(f : A \to B\)
  and \(g : B \to A\) such that they are both inverse of each other.
}
The axiom is a bit more precise in that it states that the trivial map
taking an equality of types \(A = B\) to an equivalence \(A \cong B\)
is itself an equivalence.
Univalence allows us to \emph{transport} a result onto another equivalent
setting.
An interesting consequence of the univalence axiom is that it implies
\acrshort{funext}.

Once again, this is an \emph{axiom} and as such, it does not compute.
There are ways~\sidecite[-0.4cm]{tabareau2018equivalences} to overcome this in
\Coq which has its own \acrshort{HoTT} library~\sidecite[0.2cm]{bauer2017hott}
but this still does not give a full-fledged computation rule for univalence.
To achieve this, some proof assistants implement \acrlong{CubicalTT}.

\paradot{\acrfull{CubicalTT}} \acrshort{CubicalTT}~\sidecite{cohen2016cubical}
does not build on regular type theory with an inductive equality but instead
define it diffrently, with intuition coming from cubical
sets~\sidecite[0.2cm]{coquand:cubical}.
This time the analogy with paths is put to the extreme: there is a special
construct \(\II\) morally representing the interval \([0;1]\). Its end points
are \(\Iz\) and \(\Io\).
An equality between two terms \(u\) and \(v\) of type \(A\) is now a map \(f\)
from \(\II\) to \(A\) such that \(f\ \Iz \equiv u\) and \(f\ \Io \equiv v\).
By having the map dependent instead, we can easily have heterogenous equality:
\[
  f : (i : \II) \to A\ i
\]
The main interest of this is that it gives computational content to univalence
which is no longer an axiom but a theorem. Since univalence implies
\acrshort{funext} this is also the case for it.

There is a large-scale implementation of \acrshort{CubicalTT} in
\Agda called \name{CubicalAgda}~\sidecite{vezzosi2019cubical},
amongst other efforts.

\paradot{\acrfullpl{2TT}}

\acrshort{UIP} and univalence are inconsistent together for a simple reason:
univalence allows \(\bool = \bool\) to have two \emph{distinct} proofs: one
given by the identity equivalence, the other by the equivalence that swaps
\(\ttrue\) and \(\ffalse\), while \acrshort{UIP} states there must be only one
such proof.
This in particular means that univalence is inconsistent in \acrshort{ETT}.
There is a solution to this which consists in assuming two equalities.
This cannot be done naively however, since two equalities that cohabit
collapse.
For this, the two equalities must be compartmentalised to limit their
interaction. Hence the name \acrfull{2TT}.
This idea was first introduced as \acrfull{HTS}~\sidecite[-0.4cm]{hts-sota} in
an extensional setting, but since then there are other approaches to
\acrshort{2TT}~\sidecite{DBLP:journals/corr/AltenkirchCK16,DBLP:journals/corr/AnnenkovCK17}
where only \acrshort{UIP} is assumed.

\subsection{Quotients}

Implementing quotients is a notoriously problematic issue in type theory.
In \Coq this is still limited to specific cases where the quotient can
effectively be expressed.
\acrshort{OTT} and \acrshort{STT} should in principle help in solving that
problem since they involve the idea that a type comes with its equality.
The problem is usually to make everything compute and behave nicely.

The world of \acrshort{HoTT} has brought several notions like that of
\acrfull{QIT}~\sidecite{altenkirch2016type}.
A \acrfull{QIT} not only has term constructors, but also equality constructors:
defining the type and quotienting it at the same time.